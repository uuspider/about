---
layout: post
title: Neural Network & Keras
---

<script type="text/javascript" src="{{site.baseurl}}/MathJax-2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Neural Network & Keras

参考：[Keras Documentation][ref1]{:target="_blank"}, [Keras文档][ref2]{:target="_blank"}, [卷积神经网络][ref3]{:target="_blank"}

[ref1]:https://keras.io/
[ref2]:http://keras-cn.readthedocs.io/en/latest/
[ref3]:https://www.zybuluo.com/hanbingtao/note/485480

<h2 id="top"></h2>

***

*   [神经网络](#neural_network)
    *   [感知器](#perceptron)
    *   [感知器的训练](#perceptron_train)
    *   [神经网络](#neural_network_m)
    *   [error](#)
*   [error](#)
    *   [error](#): [e](#) \| [e](#) \| [e](#)


***

神经网络是机器学习算法的一种。神经网络以感知器(Perceptron)为基本单元，通过输入层接收数据，通过输出层输出数据，输入层和输出层之间的层称为隐藏层。隐藏层比较多的神经网络被称为深度神经网络，深度学习是指使用深层架构(深度神经网络是其中的一种)的机器学习算法。

深层网络的表达力更强。一个仅有一个隐藏层的神经网络就能拟合任何一个函数，但是它需要很多神经元。而深层网络用较少的神经元就能拟合同样的函数。也就是为了拟合一个函数，要么使用一个浅而宽的网络，要么使用一个深而窄的网络，后者往往更节约资源。

训练深层网络需要大量数据，同时还需要很多技巧。

## 安装

本文使用keras和tensorflow。

    $ pip install keras
    $ pip install tensorflow
    $ python
    >>> import os
    >>> os.environ['KERAS_BACKENG']='tensorflow'
    Using TensorFlow backend.

也可以使用Theano作为backend。

## 神经网络 {#neural_network}

### 神经元 {#perceptron}

一个感知器可以接收多个输入\\((x_1, x_2, x_3, \cdots, x_n)\\)，每个输入上有一个权值\\((\omega _1, \omega _2, \omega _3, \cdots, \omega _n)\\)，此外输入层还有一个偏置项\\(\omega _0 = b\\)。

感知器需要有一个激活函数\\(f(z)\\)，可以是Sigmoid、TanH、ReLu等形式：

\\[Sigmoid(z)=\frac{1}{1+e^{-z}}\\]

\\[TanH(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}\\]

\\[ReLu(z)=max(0,z)\\]

感知器的输出可记为：

\\[y=f(\omega ^T \cdot x + b)\\]

神经元和感知器实际上是一样的，习惯上，激活函数为阶跃函数时称为感知器，而激活函数为sigmoid或tanh函数时称为神经元。

### 神经元的训练 {#perceptron_train}

神经元的训练算法与Logistic回归的训练类似，都是已知一系列的\\(x\\)和\\(y\\)，确定系数\\(\omega\\)，记\\(e=x \omega -y\\)，采用梯度上升算法，\\(\omega\\)的迭代式为：

\\[\omega:=\omega+ \eta x^T e\\]

\\(\eta\\)是步长，也被称为学习速率。当达到指定迭代次数或迭代误差时，即可确定\\(\omega\\)，完成神经元的训练，此时输入一个新的\\(x\\)，神经元给出一个\\(y\\)的预测值。

### 神经网络 {#neural_network_m}

多个神经元按照一定的规则连接起来，就构成了神经网络。对于全连接神经网络(Full Connected Neural Network)，这些规则包括：

- 感知器按照层来布局，包括输入层、输出层和隐藏层。

- 同一层的感知器之间没有连接。

- 第n层的每个感知器和第n-1层的所有感知器相连，第n-1层感知器的输出就是第n层感知器的输入。

- 每个连接都有一个权值。

下文中将要论述的卷积神经网络(CNN)、循环神经网络(RNN)则有着不同的连接规则。

假设一个全连接神经网络共有n个隐藏层，该网络的输入记为向量\\(x\\)，输出记为向量\\(y\\)，第一个隐藏层的输出记为向量\\(a_1\\)，从输入层到第一个隐藏层的权重记为矩阵\\(W_1\\)，第二个隐藏层的输出记为向量\\(a_2\\)，第一个隐藏层到第二个隐藏层的权重记为矩阵\\(W_2\\)，第n个隐藏层的输出记为向量\\(a_n\\)，从第n-1到第n个隐藏层的权重记为\\(W_n\\)，则有

\\[a_1=f(W_1 \cdot x)\\]

\\[a_2=f(W_2 \cdot a_1)\\]

\\[a_n=f(W_n \cdot a_{n-1})\\]

\\[y=f(W_{n+1} \cdot a_n)\\]

这就是神经网络输出值的计算方法。
补充说明：某一层的输出向量维数与该层神经元个数相同，权重矩阵的维数为(上层神经元个数+1)*(本层神经元个数)，这里的“+1”是因为隐藏层和输出层的神经元都会设置一个偏置项。

神经网络的训练就是确定权重矩阵的过程，这些权值称为神经网络的参数；而神经网络的连接方式、网络的层数、每层的节点数则需要人为预先设置，称为超参数。

### 反向传播算法 {#back_propagation}

如果神经元的激活函数为sigmoid函数，首先可以计算该神经网络输出层节点\\(i\\)的误差为：

\\[ \delta _i = y_i(1-y_i)(\hat y_i-y_i) \\]

其中，\\(\hat y_i\\)是节点\\(i\\)的目标值，\\(y_i\\)是节点\\(i\\)的输出值。

得到输出层误差项后，可逐层计算隐藏层节点\\(i\\)的误差：

\\[ \delta _i = a_i(1-a_i)\Sigma \omega _{ki}\delta _k  \\]

\\(a_i\\)是节点\\(i\\)的输出值，\\(\omega _{ki}\\)是节点\\(i\\)到其下一层节点\\(k\\)的连接权值，\\(\delta _k\\)是节点\\(i\\)的下一层节点\\(k\\)的误差项。

最后，更新每个连接上的权值：

\\[ \omega _{ji} := \omega _{ji} + \eta \delta _j x _{ji} \\]

其中，\\( \omega _{ji} \\)是节点\\(i\\)到节点\\(j\\)的权重，\\(\eta\\)是学习速率，\\(\delta _j\\)是节点\\(j\\)的误差项，\\( x _{ji} \\)是节点\\(i\\)传递给节点\\(j\\)的输入。该公式中的\\( - \delta _j x _{ji} \\)就是算法计算出的梯度。

可以看到，计算一个节点的误差需要先计算与之相连的下一层节点的误差，对于整个神经网络来说，需要首先计算输出层的误差，然后反向依次计算每个隐藏层的误差，直到与输入层相连的那个隐藏层，这就是“反向传播算法”的含义。所有节点的误差项计算完毕，就可以更新所有连接的权重了。

关于反向传播算法的推导，可查看[这里](https://www.zybuluo.com/hanbingtao/note/476663){:target="_blank"}。

### 梯度检查 {#check}

\\[ \omega _{ji} := \omega _{ji} + \eta \delta _j x _{ji} \\]

上述公式中给出的梯度值为\\( - \delta _j x _{ji} \\)。

梯度算法的迭代原理为：

\\[ \omega _{ji} := \omega _{ji} + \eta \frac{\partial f (\omega _{ji})}{\partial \omega _{ji}} \\]

根据导数定义：

\\[ \frac{\partial f (\omega _{ji})}{\partial \omega _{ji}} = \lim _{ \epsilon \rightarrow 0} \frac{f( \omega _{ji} + \epsilon )-f( \omega _{ji} - \epsilon )}{2 \epsilon} \\]

如果把\\( \epsilon \\)设置为一个很小的数，如0.0001，则有：

\\[ \frac{\partial f (\omega _{ji})}{\partial \omega _{ji}} \approx \frac{f( \omega _{ji} + \epsilon )-f( \omega _{ji} - \epsilon )}{2 \epsilon} \\]

对比\\( \frac{f( \omega _{ji} + \epsilon )-f( \omega _{ji} - \epsilon )}{2 \epsilon} \\)和\\( - \delta _j x _{ji} \\)，如果两者相差很小，就说明模型是准确的。

### 回归 {#regressor}

首先加载相关模块：

    from keras.models import Sequential
    from keras.layers import Dense
    import matplotlib as plt

训练集的可视化：

    plt.scatter(X,y)
    plt.show()

建立神经网络：

    model_r = Sequential()
    model_r.add(Dense(units=1,input_dim=1))

`model_r.add`是添加层，此处添加了Dense全连接层，输入和输出数据的维度都是1。

指定损失函数和优化方法：

    model_r.compile(loss='mse',optimizer='sgd')

开始训练：

    model_r.train_on_batch(X,y)

模型评估：

    model_r.evaluate(X_test,y_test,batch_size=40)

模型参数：

    model_r.layers[0].get_weights()

返回权值和偏置项。

回归曲线：

    y_pred = model_r.predict(X_test)
    plt.scatter(X_test,y_test)
    plt.plot(X_test,y_pred)
    plt.show()

### 分类 {#classification}

加载相关模块：

    from keras.models import Sequential
    from keras.layers import Dense, Activation
    from keras.optimizers import RMSprop

建立神经网络：

    model_c = Sequential([Dense(32,input_dim=784),Activation('relu'),Dense(10),Activation('softmax'),])

建立网络时直接添加层，其中第一层有32个输出维度，784个输入维度，激活函数为`relu`，第二层接收上一层输入的32个维度，输出10个维度给`softmax`。

定义优化器：

    rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)

指定损失函数、优化方法和评估参数：

    model_c.compile(optimizer=rmsprop,loss='categorical_crossentropy',metrics=['accuracy'])

训练模型：

    model_c.fit(X,y,epochs=2,batch_size=32)

模型评估：

    model_c.evaluate(X_test,y_test)

将给出误差和准确度。

## 卷积神经网络 {#convolutional_neural_network}

卷积神经网络(Convolutional Neural Network, CNN)是目前神经网络领域的一个热点，主要用于图像、语音识别，Google的GoogleNet、AlphaGo、微软的ResNet等都用到了CNN算法。

### Relu函数 {#relu}

CNN算法采用的激活函数是ReLu函数：

\\[ReLu(z)=max(0,z)\\]

ReLu函数有很多优势，如：

- 计算量小，sigmoid函数需要计算指数和倒数，而ReLu函数则不需要。

- 缓解梯度消失，sigmoid导数最大值为1/4，反向传播算法计算梯度时，每经过一层sigmoid神经元，梯度就要乘上一次sigmoid导数，梯度就会越来越小，对于深度网络就会发生所谓梯度消失的现象，而ReLu导数为1，不会引起梯度变小，因此，使用ReLu激活函数可以训练更深的网络。

- 稀疏性，采用sigmoid激活函数的神经网络，约有50%的神经元会被激活，而ReLu函数在输入小于0时是完全不激活的，因此神经元的激活率更低，节省计算资源。








**[[TOP](#top)]**

***
