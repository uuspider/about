---
layout: post
title: Neural Network & Keras
---

<script type="text/javascript" src="{{site.baseurl}}/MathJax-2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Neural Network & Keras

参考：[Keras Documentation][ref1]{:target="_blank"}, [Keras Githua][ref6]{:target="_blank"}, [Keras文档][ref2]{:target="_blank"}, [卷积神经网络][ref3]{:target="_blank"}, [机器学习][ref4]{:target="_blank"}, [Images Datasets][ref5]{:target="_blank"}, [Deep Learning Papers][ref7]{:target="_blank"}

[ref1]:https://keras.io/
[ref2]:http://keras-cn.readthedocs.io/en/latest/
[ref3]:https://www.zybuluo.com/hanbingtao/note/485480
[ref4]:http://www.jianshu.com/p/28f02bb59fe5
[ref5]:http://blog.csdn.net/qq_14845119/article/details/51913171
[ref6]:https://github.com/fchollet/keras/tree/master/keras
[ref7]:https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap

<h2 id="top"></h2>

***

*   [安装keras](#setup)
*   [神经网络](#neural_network)
    *   [神经元](#perceptron)
    *   [神经网络](#neural_network_m)
    *   [反向传播算法](#back_propagation)
    *   [激活函数](#activation): [ReLU函数](#relu) \| [Swish函数](#swish) \| [softmax函数](#softmax)
    *   [回归](#regressor)
    *   [分类](#classification)
*   [卷积神经网络](#convolutional_neural_network)
    *   [从全连接到卷积](#dese_cnn)
    *   [CNN的架构](#cnn_stucture)
    *   [卷积层输出值的计算](#con_output)
    *   [池化层输出值的计算](#pool_output)
    *   [卷积神经网络的训练](#cnn_train)
    *   [CNN分类](#cnn_c)

***

神经网络是机器学习算法的一种。神经网络以感知器(Perceptron)为基本单元，通过输入层接收数据，通过输出层输出数据，输入层和输出层之间的层称为隐藏层。隐藏层比较多的神经网络被称为深度神经网络，深度学习是指使用深层架构(深度神经网络是其中的一种)的机器学习算法。

深层网络的表达力更强。一个仅有一个隐藏层的神经网络就能拟合任何一个函数，但是它需要很多神经元。而深层网络用较少的神经元就能拟合同样的函数。也就是为了拟合一个函数，要么使用一个浅而宽的网络，要么使用一个深而窄的网络，深而窄的网络往往更节约资源。

训练深层网络需要大量数据，同时还需要很多技巧，这是因为深度学习仍然在快速发展中，各种算法的系统性仍有待归纳，最新的成果可查看这些[论文][ref7]{:target="_blank"}。

## 安装keras {#setup}

本文使用keras和tensorflow构建神经网络。

    $ pip install keras
    $ pip install tensorflow
    $ python
    >>> import os
    >>> os.environ['KERAS_BACKENG']='tensorflow'
    Using TensorFlow backend.

也可以使用Theano作为backend。

**[[TOP](#top)]**

***

## 从最小二乘到线性回归 {#to_linear_regression}

### 最小二乘法 {#least_squares}

1805年，法国数学家勒让德(Adrien-Marie Legendre)发表了《计算彗星轨道的新方法》，提出了最小二乘法。

    # y = mx + b
    # m is slope, b is y-intercept
    def compute_error_for_line_given_points(b, m, coordinates):
        totalError = 0
        for i in range(0, len(coordinates)):
            x = coordinates[i][0]
            y = coordinates[i][1]
            totalError += (y - (m * x + b)) ** 2
        return totalError / float(len(coordinates))
    # example
    compute_error_for_line_given_points(1, 2, [[3,6],[6,9],[12,18]])


![最小二乘](https://blog.floydhub.com/782eb392f13ede0e379516d4566d7aa1.svg)

通过修改m和b，使预测值\\((m*x+b)\\)和实际值\\(y\\)的误差平方和逐渐减小。这其中已经包含了机器学习的核心思想：给定输入值和期望的输出值，然后寻找两者之间的相关性。

Legendre的算法需要反复修改m和b并计算误差平方，非常耗时。

### 梯度下降 {#gradient_descent}

1909年，荷兰的Peter Debye简化了Legendre的方法。用Y表示X误差，Legendre的算法是找到使误差最小的X，在下图中，可以看到当X=1.1时，误差Y取到最小值。

![梯度下降](https://blog.floydhub.com/5b4cc6cae35385b97c3c38b1e1e66002.svg)

Debye注意到最低点左边的斜率是负的，而另一边则是正的。因此，如果知道了任意给定X的斜率值，就可以找到Y的最小值点。这就是梯度下降算法的基本思想，几乎所有的机器学习模型都会用到梯度下降算法。

假设误差函数为：

\\[Error = x ^5 -2x^3-2 \\]

这是一个极值问题，对其求导即可：

\\[Error' = 5 x ^4 - 6 x^2 \\]

    current_x = 0.5 # the algorithm starts at x=0.5
    learning_rate = 0.01 # step size multiplier
    num_iterations = 60 # the number of times to train the function

    #the derivative of the error function (x**4 = the power of 4 or x^4)
    def slope_at_given_x_value(x):
       return 5 * x**4 - 6 * x**2

    # Move X to the right or left depending on the slope of the error function
    for i in range(num_iterations):
       previous_x = current_x
       current_x += -learning_rate * slope_at_given_x_value(previous_x)
       print(previous_x)

    print("The local minimum occurs at %f" % current_x)

通过向斜率相反方向去寻找x使误差逼近最低点，也就是在`learning_rate`前需要加一个负号。

### 线性回归 {#linear_regression}

最小二乘法配合梯度下降算法，就是线性回归过程。在20世纪50到60年代，一批实验经济学家在早期的计算机上实现了这些想法。

    #Price of wheat/kg and the average price of bread
    wheat_and_bread = [[0.5,5],[0.6,5.5],[0.8,6],[1.1,6.8],[1.4,7]]

    def step_gradient(b_current, m_current, points, learningRate):
        b_gradient = 0
        m_gradient = 0
        N = float(len(points))
        for i in range(0, len(points)):
            x = points[i][0]
            y = points[i][1]
            b_gradient += -(2/N) * (y - ((m_current * x) + b_current))
            m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))
        new_b = b_current - (learningRate * b_gradient)
        new_m = m_current - (learningRate * m_gradient)
        return [new_b, new_m]

    def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):
        b = starting_b
        m = starting_m
        for i in range(num_iterations):
            b, m = step_gradient(b, m, points, learning_rate)
        return [b, m]

    gradient_descent_runner(wheat_and_bread, 1, 1, 0.01, 100)

将梯度下降运用到误差函数上，是上述算法的的一个关键点。

**[[TOP](#top)]**

***

## 神经网络 {#neural_network}

### 神经元 {#perceptron}

1943年，心理学家Warren McCulloch和数学家Walter Pitts参考生物神经元的结构，发表了抽象的M-P神经元模型，并证明了单个神经元可以执行逻辑功能。

1957年，Frank Rosenblatt以M-P模型为基础，提出了[感知器(perceptron)模型](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf){:target="_blank"}，并因此登上了《纽约时报》的头条([New Navy Device Learns By Doing](http://www.nytimes.com/1958/07/08/archives/new-navy-device-learns-by-doing-psychologist-shows-embryo-of.html){:target="_blank"})。

感知器模型沿用至今，下面是一个具有“或”逻辑功能的感知器。

    from random import choice
    from numpy import array, dot, random
    result_or = lambda x: 0 if x < 0 else 1
    training_data = [ (array([0,0,1]), 0),
                        (array([0,1,1]), 1),
                        (array([1,0,1]), 1),
                        (array([1,1,1]), 1), ]
    weights = random.rand(3)
    errors = []
    learning_rate = 0.2
    num_iterations = 100

    for i in range(num_iterations):
        input, truth = choice(training_data)
        result = dot(weights, input)
        error = truth - 1_or_0(result)
        errors.append(error)
        weights += learning_rate * error * input

    for x, _ in training_data:
        result = dot(x, weights)
        print("{}: {} -> {}".format(input[:2], result, result_or(result)))

1969年，MIT AI实验室的Marvin Minsky和Seymour Papert指出感知器无法进行“异或”的运算，还批判了多层感知器构建神经网络模型的想法。尽管此后有人提出新的理论和模型，相关的研究均没有被主流重视，神经网络开始了长达十多年的低潮期。

### 反向传播算法 {#back_propagation}

1986年，D.E.Rumelhart等人在多层神经网络模型的基础上，提出了多层神经网络权值修正的反向传播学习算法——BP算法(Error Back-Propagation)，解决了多层前向神经网络的学习问题，证明了多层神经网络具有很强的学习能力。

感知器由一组连接、一个求和单元和一个激活函数构成。感知器可以接收多个输入\\((x_1, x_2, x_3, \cdots, x_n)\\)，每个输入上有一个权值\\((\omega _1, \omega _2, \omega _3, \cdots, \omega _n)\\)，此外输入层还有一个偏置项\\(\omega _0 = b\\)，相当于线性函数中的常数项。感知器的输出可记为：

\\[y=f(\omega ^T \cdot x + b)\\]

感知器需要有一个激活函数\\(f(z)\\)，可以是sgn、sigmoid、TanH(双曲正切)等形式：

\\[Sigmoid(z)=\frac{1}{1+e^{-z}}\\]

\\[TanH(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}\\]

激活函数赋予感知器非线性映射能力。

机器学习所说的神经元和感知器实际上是一样的，习惯上，激活函数为阶跃函数(sgn)时称为感知器，而激活函数为sigmoid或tanh等连续函数时称为神经元。

多个神经元按照一定的规则连接起来，就构成了神经网络。

神经网络结构可被划分为层次型网络和互连型网络，层次型网络结构内只有相邻层的各节点之间存在连接，同层以及跨层的节点之间无连接。互连型网络结构中节点之间的连接无任何限制，任意两个节点都可以相连。

按照网络信息的流向，神经网络也可以划分为前馈型网络和反馈型网络，前馈型网络中前一层的输出作为下一层的输入，信息逐层传递，反馈型网络中所有节点不仅可以接受外界的输入信息，还能对外界输出信息。

神经网络由一个输入层、一个输出层和n个隐藏层构成，如将一个神经网络的输入记为向量\\(x\\)，输出记为向量\\(y\\)，第一个隐藏层的输出记为向量\\(a_1\\)，从输入层到第一个隐藏层的权重记为矩阵\\(W_1\\)，第二个隐藏层的输出记为向量\\(a_2\\)，第一个隐藏层到第二个隐藏层的权重记为矩阵\\(W_2\\)，第n个隐藏层的输出记为向量\\(a_n\\)，从第n-1到第n个隐藏层的权重记为\\(W_n\\)，则有

\\[a_1=f(W_1 \cdot x)\\]

\\[a_2=f(W_2 \cdot a_1)\\]

\\[a_n=f(W_n \cdot a_{n-1})\\]

\\[y=f(W_{n+1} \cdot a_n)\\]

这就是神经网络输出值的计算方法。

补充说明：某一层的输出向量维数与该层神经元个数相同，权重矩阵的维数为(上层神经元个数+1)*(本层神经元个数)，这里的“+1”是因为隐藏层和输出层的神经元都会设置一个偏置项。

神经网络的训练就是确定权重矩阵的过程，这些权值称为神经网络的参数；而神经网络的连接方式、网络的层数、每层的节点数则需要人为预先设置，称为超参数。

已知一组\\(x\\)和\\(y\\)，确定系数\\(\omega\\)，记\\(e=x \omega -y\\)，采用梯度下降算法，\\(\omega\\)的迭代式为：

\\[\omega:=\omega+ \eta x^T e\\]

\\(\eta\\)是步长，也被称为学习速率。当达到指定迭代次数或迭代误差时，即可确定\\(\omega\\)，完成神经元的训练，此时输入一个新的\\(x\\)，神经元给出一个\\(y\\)的预测值。

如果神经元的激活函数为sigmoid函数，首先可以计算该神经网络输出层节点\\(i\\)的误差为：

\\[ \delta _i = y_i(1-y_i)(\hat y _i-y_i) \\]

其中，\\(\hat y _i\\)是节点\\(i\\)的目标值，\\(y_i\\)是节点\\(i\\)的输出值。

得到输出层误差项后，可逐层计算隐藏层节点\\(i\\)的误差：

\\[ \delta _i = a_i(1-a_i)\Sigma \omega _{ki}\delta _k  \\]

\\(a_i\\)是节点\\(i\\)的输出值，\\(\omega _{ki}\\)是节点\\(i\\)到其下一层节点\\(k\\)的连接权值，\\(\delta _k\\)是节点\\(i\\)的下一层节点\\(k\\)的误差项。

最后，更新每个连接上的权值：

\\[ \omega _{ji} := \omega _{ji} + \eta \delta _j x _{ji} \\]

其中，\\( \omega _{ji} \\)是节点\\(i\\)到节点\\(j\\)的权重，\\(\eta\\)是学习速率，\\(\delta _j\\)是节点\\(j\\)的误差项，\\( x _{ji} \\)是节点\\(i\\)传递给节点\\(j\\)的输入。该公式中的\\( \delta _j x _{ji} \\)就是算法计算出的梯度。

可以看到，计算一个节点的误差需要先计算与之相连的下一层节点的误差，对于整个神经网络来说，需要首先计算输出层的误差，然后反向依次计算每个隐藏层的误差，直到与输入层相连的那个隐藏层，这就是“反向传播算法”的含义。所有节点的误差项计算完毕，就可以更新所有连接的权重了。

关于反向传播算法的推导，可查看[这里](https://www.zybuluo.com/hanbingtao/note/476663){:target="_blank"}。

我们来看看反向传播是如何解决“异或”运算的。

    import numpy as np

    X_XOR = np.array([[0,0,1], [0,1,1], [1,0,1],[1,1,1]])
    y_truth = np.array([[0],[1],[1],[0]])

    np.random.seed(1)
    syn_0 = 2*np.random.random((3,4)) - 1
    syn_1 = 2*np.random.random((4,1)) - 1

    def sigmoid(x):
        output = 1/(1+np.exp(-x))
        return output
    def sigmoid_output_to_derivative(output):
        return output*(1-output)

    for j in range(60000):
        layer_1 = sigmoid(np.dot(X_XOR, syn_0))
        layer_2 = sigmoid(np.dot(layer_1, syn_1))
        error = layer_2 - y_truth
        layer_2_delta = error * sigmoid_output_to_derivative(layer_2)
        layer_1_error = layer_2_delta.dot(syn_1.T)
        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)
        syn_1 -= layer_1.T.dot(layer_2_delta)
        syn_0 -= X_XOR.T.dot(layer_1_delta)

    print("Output After Training: \n", layer_2)

该模型同时使用了反向传播、矩阵乘法、梯度下降，解决了神经网络的“异或”问题，神经网络又被重新重视起来，一时间，上百种神经网络模型被提出来。训练神经网络需要的计算量很大，局部最优解的存在和隐藏层节点超参数进一步增加了训练难度。同时期发展起来的支持向量机，计算量相对较低，并且很容易获得全局最优解，也没有超参数，在工程应用上迅速超越神经网络。90年代中期，神经网络的研究再次进入低潮期。

现在，让我们稍微停顿一下，来看看神经网络如何处理回归和分类问题。

### 回归 {#regressor}

首先加载相关模块：

    from keras.models import Sequential
    from keras.layers import Dense
    import matplotlib as plt

`Sequential`用于逐层建立神经网络，可以一层一层地添加，`Dense`表示该神经层为全连接层。

训练集的可视化：

    plt.scatter(X,y)
    plt.show()

建立神经网络：

    model_r = Sequential()
    model_r.add(Dense(units=1,input_dim=1))

`model_r.add`是添加层，此处添加了Dense全连接层，输入和输出数据的维度都是1。

指定损失函数和优化方法并编译：

    model_r.compile('sgd','mse')

`optimizer`和`loss`是`compile`必要的两个参数。keras源码中`compile`的定义是这样的：

    compile(self, optimizer, loss, metrics=None, sample_weight_mode=None)

`optimizer`： 字符串，指定优化器。下文将会详细讨论优化器的种类、作用和选择方法。

`loss`： 字符串，损失函数，包括`mse`，`mae`，`mape`，`msle`，`kld`，`cosine`等。

`metrics`： 列表，模型的评估指标，典型用法是`metrics=['accuracy']`。

`sample_weight_mode`： 样本权重，默认为`None`，可选`temporal`，表示按时间步为样本赋权(2D权矩阵)。

开始训练：

    model_r.train_on_batch(X_train,y_train)

模型评估：

    model_r.evaluate(X_test,y_test,batch_size=40)

模型参数：

    model_r.layers[0].get_weights()

返回权值和偏置项。

回归曲线：

    y_pred = model_r.predict(X_test)
    plt.scatter(X_test,y_test)
    plt.plot(X_test,y_pred)
    plt.show()

**建立**，**编译**，**训练**，**评估**是使用keras解决问题的四个基本步骤。

### 分类 {#classification}

加载相关模块：

    from keras.models import Sequential
    from keras.layers import Dense, Activation
    from keras.optimizers import RMSprop

建立神经网络：

    model_c = Sequential([Dense(32,input_dim=784),Activation('relu'),Dense(10),Activation('softmax'),])

建立网络时直接添加层，其中第一层有32个输出维度，784个输入维度，激活函数为`relu`，第二层接收上一层输入的32个维度，输出10个维度给`softmax`。

定义优化器，可以加速训练：

    rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)

指定损失函数、优化方法和评估参数并编译：

    model_c.compile(optimizer=rmsprop,loss='categorical_crossentropy',metrics=['accuracy'])

训练模型：

    model_c.fit(X_train,y_train,epochs=2,batch_size=32)

模型评估：

    model_c.evaluate(X_test,y_test)

将给出误差和准确度。

**[[TOP](#top)]**

***

## 深度学习网络 {#deep_learning}

从90年代中期，神经网络再次进入低潮期，但仍有几位学者试图寻找突破点，2006年，加拿大多伦多大学的Geoffery Hinton教授正式提出了“深度学习”的概念，引入了“预训练”(pre-training)，使神经网络快速找到一个接近最优解的权重矩阵，之后再对其进行“微调”(fine-tuning)，这种方法大幅度减少了训练时间。

2012年，卷积神经网络(Convolutional Neural Network, CNN)在图像识别领域的突出表现，正式引爆了深度学习的研究。神经网络模型的层数逐年增加，各种优化方法层出不穷。

### 激活函数 {#activation}

感知器使用的激活函数是sgn函数，两层神经网络使用的最多的是sigmoid函数，深度网络则使用更容易使结果收敛的ReLU(Rectified Linear Unit, 修正线性单元)函数。

### ReLU函数 {#relu}

ReLU函数的表达式为：

\\[ReLU(z)=max(0,z)\\]

ReLU函数有很多优势，如sigmoid函数需要计算指数和倒数，而ReLU函数则不需要，计算量大幅减小；sigmoid导数最大值为1/4，反向传播算法计算梯度时，每经过一层sigmoid神经元，梯度就要乘上一次sigmoid导数，梯度就会越来越小，对于深度网络就会发生所谓**梯度消失**的现象，而ReLU导数为1，不会引起梯度变小，因此，使用ReLU激活函数可以训练更深的网络；采用sigmoid激活函数的神经网络，约有50%的神经元会被激活，而ReLU函数在输入小于0时是完全不激活的，因此神经元的激活率更低，节省了计算资源。

### Swish函数 {#swish}

Google Brain于最近(2017.10)提出了一种新的激活函数：

\\[Swish(z)=z \cdot sigmoid(z)\\]

其导数为：

\\[Swish'(z)=Swish(z)+sigmoid(z)(1-Swish(z))\\]

经测试，该函数比ReLU更适于处理深度学习问题。

### softmax函数 {#softmax}

假如有\\(a\\)，\\(b\\)两个数，如果\\(a>b\\)，则\\(max(a,b)\\)会直接取\\(a\\)，有时候，我们希望依据概率从\\(a\\)，\\(b\\)中取值，softmax就是这样一种函数：

\\[softmax(z) _i = \frac{e ^{z _i}}{ \Sigma _{k=1} ^{K} e ^{z _{k}}} \\]

这样就可以实现\\(K\\)元分类，上式表示将\\(z\\)分到类别\\(i\\)的概率。

### 优化器 {#optimizer}

这里的“优化”是指对梯度下降算法和反向传播算法的优化，keras内置了`sgd`， `rmsprop`， `adagrad`， `adadelta`， `adam`， `adamax`， `nadam`等优化器，可查看[这里](http://www.jianshu.com/p/d99b83f4c1a6){:target="_blank"}。

如果数据是稀疏的，就选用Adagrad， Adadelta， RMSprop， Adam，尤其是Adam，在RMSprop的基础上加了bias-correction和momentum，几乎是目前最好用的优化器。


深度学习的泛化通常被称为正则化技术。深度网络的层数增加，参数也大量增加，其表达能力大幅度增强，很容易出现过拟合现象，Dropout、数据扩容(Data-Augmentation)是目前使用最多的正则化技术。





## 卷积神经网络 {#convolutional_neural_network}

卷积神经网络(Convolutional Neural Network, CNN)是目前神经网络领域的一个热点，主要用于图像、语音识别，Google的GoogleNet、AlphaGo、微软的ResNet等都用到了CNN算法。

练习数据集：

[CIFAR-10, CIFAR-100](http://www.cs.toronto.edu/~kriz/cifar.html){:target="_blank"}

> CIFAR是由加拿大先进技术研究院的AlexKrizhevsky, Vinod Nair和Geoffrey Hinton收集的小图片数据集，包含CIFAR-10和CIFAR-100两个数据集。 CIFAR-10由60000张32*32的RGB彩色图片构成，共10个分类。50000张训练，10000张测试（交叉验证）。这个数据集最大的特点在于将识别迁移到了普适物体，而且应用于多分类。CIFAR-100由60000张图像构成，包含100个类别，每个类别600张图像，其中500张用于训练，100张用于测试。其中这100个类别又组成了20个大的类别，每个图像包含小类别和大类别两个标签。官网提供了Matlab,C，python三个版本的数据格式。

### 从全连接到卷积 {#dese_cnn}

全连接神经网络在处理图像时不可避免地遇到了以下几个问题：

- **参数数量太多** 考虑一个输入1000\*1000像素的图片(一百万像素，现在已经不能算大图了)，输入层有1000\*1000=100万节点。假设第一个隐藏层有100个节点(这个数量并不多)，那么仅这一层就有(1000\*1000+1)\*100=1亿参数，这实在是太多了！我们看到图像只扩大一点，参数数量就会多很多，因此它的扩展性很差。

- **没有利用像素之间的位置信息** 对于图像识别任务来说，每个像素和其周围像素的联系是比较紧密的，和离得很远的像素的联系可能就很小了。如果一个神经元和上一层所有神经元相连，那么就相当于对于一个像素来说，把图像的所有像素都等同看待，这不符合前面的假设。当我们完成每个连接权重的学习之后，最终可能会发现，有大量的权重，它们的值都是很小的(也就是这些连接其实无关紧要)。努力学习大量并不重要的权重，这样的学习必将是非常低效的。

- **网络层数限制** 我们知道网络层数越多其表达能力越强，但是通过梯度下降方法训练深度全连接神经网络很困难，因为全连接神经网络的梯度很难传递超过3层。因此，我们不可能得到一个很深的全连接神经网络，也就限制了它的能力。

解决这些问题的思路有：

- **局部连接** 每个神经元不再和上一层的所有神经元相连，而只和一小部分神经元相连，这样可以减少很多参数。

- **权值共享** 一组连接可以共享同一个权重，而不是每个连接有一个不同的权重，这样又减少了很多参数。

- **下采样** 使用池化层来减少每层的样本数，进一步减少参数数量，同时还可以提升模型的鲁棒性(Robust，健壮，表征在异常和危险情况下系统的生存能力)。

CNN就是从这些思路出发，尽可能保留重要参数，去掉大量不重要的参数，达到了更好的学习效果。

### CNN的架构 {#cnn_stucture}

![CNN示意图](http://upload-images.jianshu.io/upload_images/2256672-a36210f89c7164a7.png)

图1 CNN示意图

如图所示，CNN由输入层(INPUT)、卷积层(CONV)、池化层(POOL, Pooling)、全连接层(FC)、输出层(OUTPUT)组成，即INPUT-[CONV\*1-POOL]\*2-FC\*2-OUTPUT。

CNN每层的神经元都是三维排列的，有长度、宽度和厚度，卷积就是将长宽减小、厚度增加的过程。

对于图1展示的神经网络，我们看到输入层的长度和宽度对应于输入图像的长度和宽度，而它的厚度为1。接着，第一个卷积层对这幅图像进行了卷积操作，得到了三个Feature Map。这里的"3"可能是让很多初学者迷惑的地方，实际上，就是这个卷积层包含三个Filter，也就是三套参数，每个Filter都可以把原始输入图像卷积得到一个Feature Map，三个Filter就可以得到三个Feature Map。至于一个卷积层可以有多少个Filter，那是可以自由设定的。也就是说，卷积层的Filter个数也是一个超参数。我们可以把Feature Map可以看做是通过卷积变换提取到的图像特征，三个Filter就对原始图像提取出三组不同的特征，也就是得到了三个Feature Map，也称做三个通道(channel)。

继续观察图1，在第一个卷积层之后，池化层对三个Feature Map做了下采样，得到了三个更小的Feature Map。接着，是第二个卷积层，它有5个Filter。每个Fitler都把前面下采样之后的3个Feature Map卷积在一起，得到一个新的Feature Map。这样，5个Filter就得到了5个Feature Map。接着，是第二个Pooling，继续对5个Feature Map进行下采样，得到了5个更小的Feature Map。

图1所示网络的最后两层是全连接层。第一个全连接层的每个神经元，和上一层5个Feature Map中的每个神经元相连，第二个全连接层(也就是输出层)的每个神经元，则和第一个全连接层的每个神经元相连，这样得到了整个网络的输出。

### 卷积层输出值的计算 {#con_output}

如图所示，有一个5\*5的图像，使用一个3\*3的filter进行卷积，步幅(stride)为1。

![卷积层输出值计算](http://upload-images.jianshu.io/upload_images/2256672-548b82ccd7977294.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640)

首先对图像的每个像素进行编号，用\\(x _{i,j}\\)表示图像的第\\(i\\)行第\\(j\\)列元素；对filter的每个权重进行编号，用\\( \omega _{m,n}\\)表示第行\\(m\\)第\\(n\\)列权重，用\\( \omega _b\\)表示filter的偏置项；对Feature Map的每个元素进行编号，用\\(a _{i,j}\\)表示Feature Map的第\\(i\\)行第\\(j\\)列元素；用\\(f\\)表示激活函数(如ReLU函数)。上图可使用下列公式计算卷积：

\\[ a _{i,j} = f(\Sigma ^2 _{m=0} \Sigma ^2 _{n=0} \omega _{m,n} x _{i+m,j+n} + \omega _b) \\]

计算过程和结果如下图：

![卷积层输出值计算](http://upload-images.jianshu.io/upload_images/2256672-19110dee0c54c0b2.gif)

如果步幅为2，则结果为：

![卷积层输出值计算](http://upload-images.jianshu.io/upload_images/2256672-7919cabd375b4cfd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640)

图像边长\\(W _1\\)，步幅\\(S\\)，filter边长\\(F\\)和卷积后Feature Map的边长\\(W _2\\)有如下关系：

\\[ W _2 = (W _1 - F + 2 P)/S +1 \\]

上式中的\\(P\\)是Zero Padding数值，Zero Padding是指在原始图像周围补几圈0，这种处理方式有助于图像边缘部分的特征提取。

上面的初始图像深度(厚度)为1，如果图像深度为\\(D\\)，则相应的filter的深度也必须是\\(D\\)，此时卷积层输出的计算公式为：

\\[ a _{d,i,j} = f(\Sigma ^{D-1} _{d=0} \Sigma ^{F-1} _{m=0} \Sigma ^{F-1} _{n=0} \omega _{d,m,n} x _{d,i+m,j+n} + \omega _b) \\]

每个卷积层可以有多个filter，每个filter和原始图像进行卷积后，都可以得到一个Feature Map，因此，卷积后Feature Map的深度和卷积层的filter个数是相同的。

下图是7\*7\*3图像，经过两个3\*3\*3filter的卷积(步幅为2)过程，得到3\*3\*2的输出，其中Zero Padding为1，即在输入元素的周围补了一圈0。

![卷积层输出值计算](http://upload-images.jianshu.io/upload_images/2256672-958f31b01695b085.gif)

可以看到，与全连接神经网络相比，CNN通过**局部连接**和**权值共享**，使参数数量大大减少了。

### 池化层输出值的计算 {#pool_output}

池化层主要的作用是下采样，通过去掉Feature Map中不重要的样本，进一步减少参数数量。Pooling的方法很多，最常用的是Max Pooling。Max Pooling实际上就是在n*n的样本中取最大值，作为采样后的样本值，如下图。除了Max Pooing之外，常用的还有Mean Pooling(取池化窗口中各样本的平均值)和Stoastic Pooling(随机选择池化窗口中的一个样本值)。

![池化层输出值计算](http://upload-images.jianshu.io/upload_images/2256672-03bfc7683ad2e3ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640)

对于深度为\\(D\\)的Feature Map，各层独立做Pooling，因此Pooling后的深度仍然为\\(D\\)。

### 卷积神经网络的训练 {#cnn_train}

和神经网络的训练方法类似，CNN也采用反向传播算法，首先将误差项从输出层逐层上传，然后计算每个参数的梯度，最后更新参数，详细的推导过程可查看[这里](https://www.zybuluo.com/hanbingtao/note/485480){:target="_blank"}。

### CNN分类 {#cnn_c}

加载相关模块：

    import numpy as np
    from keras.models import Sequential
    from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten
    from keras.optimizers import Adam

建立CNN模型：

    cnn_c = Sequential()

添加卷积层，输出为(32,28,28)：

    cnn_c.add(Convolution2D(batch_input_shape=(None,1,28,28),filter=32,kernel_size=5,strides=1,padding='same',data_format='channels_first',))
    cnn_c.add(Activation('relu'))

添加池化层，max pooling，输出为(32,14,14)：

    cnn_c.add(MaxPooling2D(pool_size=2,strides=2,padding='same',data_format='channels_first',))

添加第二个卷积层，输出为(64,14,14)：

    cnn_c.add(Convolution2D(64,5,strides=1,padding='same',data_format='channels_first'))
    cnn_c.add(Activation('relu'))

添加池化层，max pooling，输出为(64,7,7)：

    cnn_c.add(MaxPooling2D(2,2,'same',data_format='channels_first'))

添加全连接层，输入为(64,7,7)，输出为1024维向量：

    cnn_c.add(Flatten())
    cnn_c.add(Dense(1024))
    cnn_c.add(Activation('relu'))

添加全连接层，输出为10维向量：

    cnn_c.add(Dense(10))
    cnn_c.add(Activation('softmax'))

定义优化器：

    adam = Adam(lr=1e-4)

指定优化方法、损失函数和评估参数并编译：

    cnn_c.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])

训练模型：

    cnn_c.fit(X_train,y_train,epochs=1,batch_size=64,)

模型评估：

    cnn_c.evaluate(X_test, y_test)

将给出误差和准确度。

**[[TOP](#top)]**

***
