---
layout: post
title: Neural Network & Keras
---

<script type="text/javascript" src="{{site.baseurl}}/MathJax-2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Neural Network & Keras

参考：[Keras Documentation][ref1]{:target="_blank"}, [Keras文档][ref2]{:target="_blank"}, [卷积神经网络][ref3]{:target="_blank"}

[ref1]:https://keras.io/
[ref2]:http://keras-cn.readthedocs.io/en/latest/
[ref3]:https://www.zybuluo.com/hanbingtao/note/485480

<h2 id="top"></h2>

***

*   [神经网络](#neural_network)
    *   [感知器](#perceptron)
    *   [感知器的训练](#perceptron_train)
    *   [神经网络](#neural_network_m)
    *   [error](#)
*   [error](#)
    *   [error](#): [e](#) \| [e](#) \| [e](#)


***

神经网络是机器学习算法的一种。神经网络以神经元——感知器(Perceptron)为基本单元，通过输入层接收数据，通过输出层输出数据，输入层和输出层之间的层称为隐藏层。隐藏层比较多的神经网络被称为深度神经网络，深度学习是指使用深层架构(深度神经网络是其中的一种)的机器学习算法。

深层网络的表达力更强。一个仅有一个隐藏层的神经网络就能拟合任何一个函数，但是它需要很多神经元。而深层网络用较少的神经元就能拟合同样的函数。也就是为了拟合一个函数，要么使用一个浅而宽的网络，要么使用一个深而窄的网络，后者往往更节约资源。

训练深层网络需要大量数据，同时还需要很多技巧。

## 神经网络 {#neural_network}

### 感知器 {#perceptron}

一个感知器可以接收多个输入\\((x_1, x_2, x_3, \cdots, x_n)\\)，每个输入上有一个权值\\((\omega _1, \omega _2, \omega _3, \cdots, \omega _n)\\)，此外输入层还有一个偏置项\\(\omega _0 = b\\)。

感知器需要有一个激活函数\\(f(z)\\)，可以是Sigmoid、TanH、ReLU等形式：

\\[Sigmoid(z)=\frac{1}{1+e^{-z}}\\]

\\[TanH(z)=\frac{e^x-e^{-z}}{e^x+e^{-z}}\\]

\\[ReLU(z)=max(0,z)\\]

感知器的输出可记为：

\\[y=f(\omega ^T \cdot x + b)\\]

### 感知器的训练 {#perceptron_train}

感知器的训练算法与Logistic回归的训练类似，都是已知一系列的\\(x\\)和\\(y\\)，确定系数\\(\omega\\)，记\\(e=x \omega -y\\)，采用梯度上升算法，\\(\omega\\)的迭代式为：

\\[\omega:=\omega+ \eta x^T e\\]

\\(\eta\\)是步长，也被称为学习速率。当达到指定迭代次数或迭代误差时，即可确定\\(\omega\\)，完成感知器的训练，此时输入一个新的\\(x\\)，感知器给出一个\\(y\\)的预测值。

### 神经网络 {#neural_network_m}

多个感知器按照一定的规则连接起来，就构成了神经网络。对于全连接神经网络(Full Connected Neural Network, FCNN)，这些规则包括：

- 感知器按照层来布局，包括输入层、输出层和隐藏层。

- 同一层的感知器之间没有连接。

- 第n层的每个感知器和第n-1层的所有感知器相连，第n-1层感知器的输出就是第n层感知器的输入。

- 每个连接都有一个权值。

下文中将要论述的卷积神经网络、循环神经网络则有与此不同的连接规则。







**[[TOP](#top)]**

***
