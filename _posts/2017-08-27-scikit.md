---
layout: post
title: Data science & scikit-learn
---

<script type="text/javascript" src="{{site.baseurl}}/MathJax-2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Data science & scikit-learn

参考：[Machine Learning Map][ref1]{:target="_blank"}, [urllib2 - extensible library for opening URLs][ref2]{:target="_blank"}

[ref1]:http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
[ref2]:https://docs.python.org/2/library/urllib2.html

<h2 id="top"></h2>

***

*   [数据导入](#input)
*   [距离计算](#distance): [Minkowski](#minkowski)\|[Euclidean](#euclidean)\|[Manhattan](#manhattan)\|[cosine](#cosine)\|[Jaccard](#jaccard)
*   [k近邻](#knn)
*   [贝叶斯分类器](#proxy)
*   [决策树](#timeout)
*   [Logistic回归](#error)
*   [支持向量机](#error)
*   [k-means聚类](#error)
*   [Bagging](#error)
*   [AdaBoost](#error)
*   [随机森林](#error)
*   [超随机树](#error)
*   [旋转森林](#error)


***

### 数据导入 {#input}

数据导入是数据处理、机器学习的第一步。首先对数据进行清洗并存储为csv格式，之后导入数据集为嵌套列表：

    import numpy as np
    dataset = np.loadtxt('x.csv', delimiter=",")

numpy的更多用法，请查看[这里](http://about.uuspider.com/2015/08/29/numpy.html)。

## 距离表征 {#distance}

### Minkowski距离 {#minkowski}

Minkowski距离的计算公式为：

\\[ d ( {\bf x}, {\bf y} ) = ( \sum_{i=1}^{n} \vert x_i - y_i \vert ^p)^\frac{1}{p} \\]


    def minkowski_distance(x,y,power):
        if len(x) == len(y):
            return np.power(np.sum(np.power((x-y),power)),(1/(1.0*power)))
        else:
            print "Input should be of equal length"
        return None

### Euclidean距离 {#euclidean}

令Minkowski距离公式中的p=2，得到Euclidean距离公式：

\\[ d ( {\bf x}, {\bf y} ) = ( \sum_{i=1}^{n} \vert x_i - y_i \vert ^2)^\frac{1}{2} \\]


    def euclidean_distance(x,y):
        return minkowski_distance(x,y,2)

### Manhattan距离 {#manhattan}

令Minkowski距离公式中的p=1，得到Manhattan距离公式：

\\[ d ( {\bf x}, {\bf y} ) = \sum_{i=1}^{n} \vert x_i - y_i \vert \\]


    def euclidean_distance(x,y):
        return minkowski_distance(x,y,1)

### cosine距离 {#cosine}

将Euclidean距离加1，然后取倒数，就可以得到一个介于0到1之间的值，可以用来表示两个向量相似的程度，1表示完全相同。

与此类似，两个向量夹角的余弦值可以更准确地判断这两个向量的相似程度，它反映的是这两个向量与某一条直线的拟合程度，因此也被称为Pearson相关系数，该系数介于-1到1之间，1表示完全正相关，0表示无关，-1表示完全负相关。Pearson相关系数常常用于文本挖掘，词是轴，文档为向量，Pearson系数就代表了这两个文档之间的相似度。

\\[ r ( {\bf x}, {\bf y} ) = \frac{\sum xy - \frac{\sum x \sum y}{n}}{\sqrt{(\sum x^2 - \frac{(\sum x)^2}{n})(\sum y^2 - \frac{(\sum y)^2}{n})}}\\]


    def cosine_distance(x,y):
        if len(x) == len(y):
            return np.dot(x,y)/np.sqrt(np.dot(x,x)*np.dot(y,y))
        else:
            print "Input should be of equal length"
        return None

对于矩阵a，`np.corrcoef(a)`和`np.corrcoef(a,rowvar=0)`可以计算各行和各列之间的相关系数，如：

    >>> a = [[1, 2, 3, 4, 5],
            [8, 3, 4, 2, 6],
            [9, 5, 7, 2, 5]]
    >>> np.corrcoef(a)
    array([[ 1.        , -0.32826608, -0.66697297],
          [-0.32826608,  1.        ,  0.80412382],
          [-0.66697297,  0.80412382,  1.        ]])

### Jaccard距离 {#jaccard}

Jaccard距离属于非欧空间距离。两个集合的交集与并集的比值称为Jaccard系数，Jaccard系数可用于衡量聚类算法的性能。1减去Jaccard系数就是Jaccard距离。


    def jaccard_distance(x,y):
        set_x = set(x)
        set_y = set(y)
        return 1-len(set_x.intersection(set_y)) / len(set_x.union(set_y))

## k近邻 {#knn}

### 数据分割 {#split}

kNN的数据集格式通常是这样的：

1. 每一行是一个实例(或记录、样本等)；

2. 每一列表示一个特征(或属性等)；

3. 最后一列是标签(或类别、标记等)。

对于有监督的kNN算法，需要将导入的数据合理分割为训练集和测试集，为使训练集和测试集中的数据随机分布，首先将数据打乱：

    >>> np.random.shuffle(dataset)
    >>> train_size = 0.8
    >>> test_size = 1-train_size
    >>> from sklearn.cross_validation import train_test_split
    >>> train, test = train_test_split(dataset,test_size=test_size)

### 查找k近邻 {#knn_main}

    from sklearn.datasets import make_classification

    import numpy as np
    import matplotlib.pyplot as plt
    import itertools

    from sklearn.ensemble import BaggingClassifier
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.cross_validation import StratifiedShuffleSplit
    from sklearn.metrics import classification_report

    def get_data():
        x,y = make_classification(n_features=4)
        return x,y

    def plot_data(x,y):
        subplot_start = 100
        col_numbers = range(0,4)
        col_pairs = itertools.combinations(col_numbers,2)
        for col_pair in col_pairs:
            plt.subplot(subplot_start)
            plt.scatter(x[:,col_pair[0]],x[:,col_pair[1]],c=y)
            title_string = str(col_pair[0]) + "-" + str(col_pair[1])
            plt.title(title_string)
            x_label = str(col_pair[0])
            y_label = str(col_pair[1])
            plt.xlabel(x_label)
            plt.xlabel(y_label)
            subplot_start+=1
        plt.show()

    def get_train_test(x,y):
        train_size = 0.8
        test_size = 1 - train_size
        input_dataset = np.column_stack([x,y])
        stratified_split = StratifiedShuffleSplit(input_dataset[:,-1],test_size=test_size,n_iter=1)
        for train_indx,test_indx in stratified_split:
            train_x = input_dataset[train_indx,:-1]
            train_y = input_dataset[train_indx,-1]
            test_x = input_dataset[test_indx,:-1]
            test_y = input_dataset[test_indx,-1]
        return train_x,train_y,test_x,test_y

    def build_model(x,y,k=2):
        knn = KNeighborsClassifier(n_neighbors=k)
        knn.fit(x,y)
        return knn

    def test_model(x,y,knn_model):
        y_predicted = knn_model.predict(x)
        print classification_report(y,y_predicted)

    if __name__ == "__main__":
        x,y = get_data()
