---
layout: post
title: Data science & scikit-learn
---

<script type="text/javascript" src="{{site.baseurl}}/MathJax-2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Data science & scikit-learn

参考：[Machine Learning Map][ref1]{:target="_blank"}, [urllib2 - extensible library for opening URLs][ref2]{:target="_blank"}

[ref1]:http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
[ref2]:https://docs.python.org/2/library/urllib2.html

<h2 id="top"></h2>

***

*   [数据导入](#input)
*   [距离计算](#distance)
    *   [Minkowski](#minkowski)
    *   [Euclidean](#euclidean)
    *   [Manhattan](#manhattan)
    *   [cosine](#cosine)
    *   [Jaccard](#jaccard)
    *   [数据归一化](#norm)
*   [k近邻](#knn)
    *   [数据分割](#split)
    *   [查找最近邻](#knn_main)
*   [贝叶斯分类器](#proxy)
*   [决策树](#timeout)
*   [Logistic回归](#error)
*   [支持向量机](#error)
*   [k-means聚类](#error)
*   [Bagging](#error)
*   [AdaBoost](#error)
*   [随机森林](#error)
*   [超随机树](#error)
*   [旋转森林](#error)


***

## 数据导入 {#input}

数据导入是数据处理、机器学习的第一步。首先对数据进行清洗并存储为csv格式，之后导入数据集为嵌套列表：

    import numpy as np
    dataset = np.loadtxt('x.csv', delimiter=",")

numpy的更多用法，请查看[这里](http://about.uuspider.com/2015/08/29/numpy.html)。

**[[TOP](#top)]**

***

## 距离表征 {#distance}

### Minkowski距离 {#minkowski}

Minkowski距离也被称为Lp距离，其计算公式为：

\\[ d ( {\bf x}, {\bf y} ) = ( \sum_{i=1}^{n} \vert x_i - y_i \vert ^p)^\frac{1}{p} \\]


    def minkowski_distance(x,y,power):
        if len(x) == len(y):
            return np.power(np.sum(np.power((x-y),power)),(1/(1.0*power)))
        else:
            print "Input should be of equal length"
        return None

### Euclidean距离 {#euclidean}

令Minkowski距离公式中的p=2，得到Euclidean距离公式：

\\[ d ( {\bf x}, {\bf y} ) = ( \sum_{i=1}^{n} \vert x_i - y_i \vert ^2)^\frac{1}{2} \\]


    def euclidean_distance(x,y):
        return minkowski_distance(x,y,2)

### Manhattan距离 {#manhattan}

令Minkowski距离公式中的p=1，得到Manhattan距离公式：

\\[ d ( {\bf x}, {\bf y} ) = \sum_{i=1}^{n} \vert x_i - y_i \vert \\]


    def euclidean_distance(x,y):
        return minkowski_distance(x,y,1)

### cosine距离 {#cosine}

将Euclidean距离加1，然后取倒数，就可以得到一个介于0到1之间的值，可以用来表示两个向量相似的程度，1表示完全相同。

与此类似，两个向量夹角的余弦值可以更准确地判断这两个向量的相似程度，它反映的是这两个向量与某一条直线的拟合程度，因此也被称为Pearson相关系数，该系数介于-1到1之间，1表示完全正相关，0表示无关，-1表示完全负相关。Pearson相关系数常常用于文本挖掘，词是轴，文档为向量，Pearson系数就代表了这两个文档之间的相似度。

\\[ r ( {\bf x}, {\bf y} ) = \frac{\sum xy - \frac{\sum x \sum y}{n}}{\sqrt{(\sum x^2 - \frac{(\sum x)^2}{n})(\sum y^2 - \frac{(\sum y)^2}{n})}}\\]


    def cosine_distance(x,y):
        if len(x) == len(y):
            return np.dot(x,y)/np.sqrt(np.dot(x,x)*np.dot(y,y))
        else:
            print "Input should be of equal length"
        return None

对于矩阵a，`np.corrcoef(a)`和`np.corrcoef(a,rowvar=0)`可以计算各行和各列之间的相关系数，如：

    >>> a = [[1, 2, 3, 4, 5],
            [8, 3, 4, 2, 6],
            [9, 5, 7, 2, 5]]
    >>> np.corrcoef(a)
    array([[ 1.        , -0.32826608, -0.66697297],
          [-0.32826608,  1.        ,  0.80412382],
          [-0.66697297,  0.80412382,  1.        ]])

### Jaccard距离 {#jaccard}

Jaccard距离属于非欧空间距离。两个集合的交集与并集的比值称为Jaccard系数，Jaccard系数可用于衡量聚类算法的性能。1减去Jaccard系数就是Jaccard距离。


    def jaccard_distance(x,y):
        set_x = set(x)
        set_y = set(y)
        return 1-len(set_x.intersection(set_y)) / len(set_x.union(set_y))

### 数据的归一化 {#norm}

对于不同的特征、属性，其取值可能相差很大，如年龄的取值一般为0到100，而行驶里程有可能在0到几万甚至几十万之间取值，这会影响距离计算结果的合理性。因此，有时候，需要对数据集进行归一化处理，也就是将所有特征、属性的取值范围转化在0到1之间。

    def norm(dataset):
        minVals = dataset.min(0)  #选取某一列的最小值
        maxVals = dataset.max(0)
        ranges = maxVals - minVals
        normDataset = np.zeros(np.shape(dataset))
        m = dataset.shape[0]
        normDataset = (dataset - np.tile(minVals,(m,1))) / np.tile(ranges,(m,1))
        return normDataset

**[[TOP](#top)]**

***

## k近邻 {#knn}

### 数据分割 {#split}

kNN的数据集格式通常是这样的：

1. 每一行是一个实例(或记录、样本等)；

2. 每一列表示一个特征(或属性等)；

3. 最后一列是标签(或类别、标记等)。

对于有监督的kNN算法，需要将导入的数据合理分割为训练集和测试集，为使训练集和测试集中的数据随机分布，首先将数据打乱：

    >>> np.random.shuffle(dataset)
    >>> train_size = 0.8
    >>> test_size = 1-train_size
    >>> from sklearn.cross_validation import train_test_split
    >>> train, test = train_test_split(dataset,test_size=test_size)

### 查找k近邻 {#knn_main}

将训练集中的特征矩阵和标签向量分开：

    for indx in train:
        x = train[indx,:-1]
        y = train[indx,-1]

指定k值创建kNN分类器，并开始训练：

    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier(n_neighbors=10)
    knn.fit(x,y)

训练完成后将返回knn对象信息：

    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
       metric_params=None, n_jobs=1, n_neighbors=10, p=2,
       weights='uniform')

这些参数都是可以指定的，如algorithm包含'ball_tree'，'kd_tree'，'brute'以及'auto'，leaf_size是一个整数，用于指定'ball_tree'、'kd_tree'叶节点的数，metric用于指定距离度量方式，n_jobs指定并行计算线程数量，-1表示CPU的内核数，n_neighbors指定k值，p指定Minkowski距离中的p值，weights是权重，'uniform'表示等比重，'distance'表示按距离反比等等。

这时候可以查看当给定x向量时，模型预测的标签：

    y_predicted = knn_model.predict(x)

对照模型预测标签和真实标签，就可以评价该模型的性能：

    from sklearn.metrics import classification_report
    print classification_report(y,y_predicted)

结果是类似这样的报告形式：

            precision    recall  f1-score   support

    class 0       0.50      1.00      0.67         1
    class 1       0.00      0.00      0.00         1
    class 2       1.00      0.67      0.80         3

    avg / total       0.70      0.60      0.61         5

其中precision表示真实标签与真实标签加漏报标签之间的比值。

查看模型的准确率：

    knn.score(x,y,sample_weight=None)

查看模型的误报情况：

    from sklearn.metrics import confusion_matrix
    print confusion_matrix(y,y_predicted)

输出为一个矩阵(Confusion Matrix)：

    array([[80,  0, 10],
           [ 0, 85,  5],
           [ 1, 6, 83]])

这个矩阵可以直观地看到模型的误报情况，以第二行为例，表示将class2误报为class3的数目为5，误报为class1的数目为0。

计算预测结果的概率分布：

    knn.predict_proba(x)

计算距离测试样本最近的10个点，返回的是这些实例所在的行序号：

    knn.kneighbors(x,10,False)

第三个参数是'return_distance'，指定是否返回具体的距离值。

**[[TOP](#top)]**

***
