---
layout: post
title: Data science & scikit-learn
---

<script type="text/javascript" src="{{site.baseurl}}/MathJax-2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Data science & scikit-learn

参考：[Machine Learning Map][ref1]{:target="_blank"}, [urllib2 - extensible library for opening URLs][ref2]{:target="_blank"}

[ref1]:http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
[ref2]:https://docs.python.org/2/library/urllib2.html

<h2 id="top"></h2>

***

*   [数据导入](#input)
*   [距离表征](#distance)
    *   [Minkowski](#minkowski)
    *   [Euclidean](#euclidean)
    *   [Manhattan](#manhattan)
    *   [cosine](#cosine)
    *   [Jaccard](#jaccard)
    *   [数据归一化](#norm)
*   [k近邻](#knn)
    *   [数据分割](#split)
    *   [查找最近邻](#knn_main)
*   [贝叶斯分类器](#bayes)
    *   [生成式模型](#generative)
    *   [文本向量化](#text2vec)
*   [决策树](#timeout)
*   [Logistic回归](#error)
*   [支持向量机](#error)
*   [k-means聚类](#error)
*   [Bagging](#error)
*   [AdaBoost](#error)
*   [随机森林](#error)
*   [超随机树](#error)
*   [旋转森林](#error)


***

## 数据导入 {#input}

数据导入是数据处理、机器学习的第一步。首先对数据进行清洗并存储为csv格式，之后导入数据集为嵌套列表：

    import numpy as np
    dataset = np.loadtxt('x.csv', delimiter=",")

numpy的更多用法，请查看[这里](http://about.uuspider.com/2015/08/29/numpy.html)。

**[[TOP](#top)]**

***

## 距离表征 {#distance}

### Minkowski距离 {#minkowski}

Minkowski距离也被称为Lp距离，其计算公式为：

\\[ d ( {\bf x}, {\bf y} ) = ( \sum_{i=1}^{n} \vert x_i - y_i \vert ^p)^\frac{1}{p} \\]


    def minkowski_distance(x,y,power):
        if len(x) == len(y):
            return np.power(np.sum(np.power((x-y),power)),(1/(1.0*power)))
        else:
            print "Input should be of equal length"
        return None

### Euclidean距离 {#euclidean}

令Minkowski距离公式中的p=2，得到Euclidean距离公式：

\\[ d ( {\bf x}, {\bf y} ) = ( \sum_{i=1}^{n} \vert x_i - y_i \vert ^2)^\frac{1}{2} \\]


    def euclidean_distance(x,y):
        return minkowski_distance(x,y,2)

### Manhattan距离 {#manhattan}

令Minkowski距离公式中的p=1，得到Manhattan距离公式：

\\[ d ( {\bf x}, {\bf y} ) = \sum_{i=1}^{n} \vert x_i - y_i \vert \\]


    def euclidean_distance(x,y):
        return minkowski_distance(x,y,1)

### cosine距离 {#cosine}

将Euclidean距离加1，然后取倒数，就可以得到一个介于0到1之间的值，可以用来表示两个向量相似的程度，1表示完全相同。

与此类似，两个向量夹角的余弦值可以更准确地判断这两个向量的相似程度，它反映的是这两个向量与某一条直线的拟合程度，因此也被称为Pearson相关系数，该系数介于-1到1之间，1表示完全正相关，0表示无关，-1表示完全负相关。Pearson相关系数常常用于文本挖掘，词是轴，文档为向量，Pearson系数就代表了这两个文档之间的相似度。

\\[ r ( {\bf x}, {\bf y} ) = \frac{\sum xy - \frac{\sum x \sum y}{n}}{\sqrt{(\sum x^2 - \frac{(\sum x)^2}{n})(\sum y^2 - \frac{(\sum y)^2}{n})}}\\]


    def cosine_distance(x,y):
        if len(x) == len(y):
            return np.dot(x,y)/np.sqrt(np.dot(x,x)*np.dot(y,y))
        else:
            print "Input should be of equal length"
        return None

对于矩阵a，`np.corrcoef(a)`和`np.corrcoef(a,rowvar=0)`可以计算各行和各列之间的相关系数，如：

    >>> a = [[1, 2, 3, 4, 5],
            [8, 3, 4, 2, 6],
            [9, 5, 7, 2, 5]]
    >>> np.corrcoef(a)
    array([[ 1.        , -0.32826608, -0.66697297],
          [-0.32826608,  1.        ,  0.80412382],
          [-0.66697297,  0.80412382,  1.        ]])

### Jaccard距离 {#jaccard}

Jaccard距离属于非欧空间距离。两个集合的交集与并集的比值称为Jaccard系数，Jaccard系数可用于衡量聚类算法的性能。1减去Jaccard系数就是Jaccard距离。


    def jaccard_distance(x,y):
        set_x = set(x)
        set_y = set(y)
        return 1-len(set_x.intersection(set_y)) / len(set_x.union(set_y))

### 数据的归一化 {#norm}

对于不同的特征、属性，其取值可能相差很大，如年龄的取值一般为0到100，而行驶里程有可能在0到几万甚至几十万之间取值，这会影响距离计算结果的合理性。因此，有时候，需要对数据集进行归一化处理，也就是将所有特征、属性的取值范围转化在0到1之间。

    def norm(dataset):
        minVals = dataset.min(0)  #选取某一列的最小值
        maxVals = dataset.max(0)
        ranges = maxVals - minVals
        normDataset = np.zeros(np.shape(dataset))
        m = dataset.shape[0]
        normDataset = (dataset - np.tile(minVals,(m,1))) / np.tile(ranges,(m,1))
        return normDataset

**[[TOP](#top)]**

***

## k近邻 {#knn}

### 数据分割 {#split}

kNN的数据集格式通常是这样的：

1. 每一行是一个实例(或记录、样本等)；

2. 每一列表示一个特征(或属性等)；

3. 最后一列是标签(或类别、标记等)。

对于有监督的kNN算法，需要将导入的数据合理分割为训练集和测试集，为使训练集和测试集中的数据随机分布，首先将数据打乱：

    >>> np.random.shuffle(dataset)
    >>> train_size = 0.8
    >>> test_size = 1-train_size
    >>> from sklearn.cross_validation import train_test_split
    >>> train, test = train_test_split(dataset,test_size=test_size)

### 查找k近邻 {#knn_main}

将训练集中的特征矩阵和标签向量分开：

    for indx in train:
        x = train[indx,:-1]
        y = train[indx,-1]

指定k值创建kNN分类器，并开始训练：

    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier(n_neighbors=10)
    knn.fit(x,y)

训练完成后将返回knn对象信息：

    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
       metric_params=None, n_jobs=1, n_neighbors=10, p=2,
       weights='uniform')

这些参数都是可以指定的，如algorithm包含'ball_tree'，'kd_tree'，'brute'以及'auto'，leaf_size是一个整数，用于指定'ball_tree'、'kd_tree'叶节点的数，metric用于指定距离度量方式，n_jobs指定并行计算线程数量，-1表示CPU的内核数，n_neighbors指定k值，p指定Minkowski距离中的p值，weights是权重，'uniform'表示等比重，'distance'表示按距离反比等等。

这时候可以查看当给定x向量时，模型预测的标签：

    y_predicted = knn_model.predict(x)

对照模型预测标签和真实标签，就可以评价该模型的性能：

    from sklearn.metrics import classification_report
    print classification_report(y,y_predicted)

结果是类似这样的报告形式：

            precision    recall  f1-score   support

    class 0       0.50      1.00      0.67         1
    class 1       0.00      0.00      0.00         1
    class 2       1.00      0.67      0.80         3

    avg / total       0.70      0.60      0.61         5

其中precision表示真实标签与真实标签加漏报标签之间的比值。

查看模型的准确率：

    knn.score(x,y,sample_weight=None)

查看模型的误报情况：

    from sklearn.metrics import confusion_matrix
    print confusion_matrix(y,y_predicted)

输出为一个矩阵(Confusion Matrix)：

    array([[80,  0, 10],
           [ 0, 85,  5],
           [ 1,  6, 83]])

这个矩阵可以直观地看到模型的误报情况，以第二行为例，表示将class2误报为class3的数目为5，误报为class1的数目为0。

计算预测结果的概率分布：

    knn.predict_proba(x)

计算距离测试样本最近的10个点，返回的是这些实例所在的行序号：

    knn.kneighbors(x,10,False)

第三个参数是'return_distance'，指定是否返回具体的距离值。

**[[TOP](#top)]**

***

## 贝叶斯分类器 {#bayes}

### 生成式模型 {#generative}

贝叶斯分类器的理论基础是贝叶斯公式：

\\[ P ( c \vert {\bf x } ) = \frac{ P ( {\bf x } \vert c ) P ( c ) }{ P ( {\bf x } ) }\\]

这里的P(c)为先验概率，P(x\|c)是样本x相对于类标记c的类条件概率，P(x)是用于归一化的因子，对于给定的样本x，P(x)与类标记无关，因此，估计P(c\|x)的问题就转化为计算P(x\|c)P(c)。

如果x中各实例的特征、属性之间相互独立，即每个特征、属性**独立**影响分类结果，因此有

\\[ P ( {\bf x } \vert c ) = \prod ^{t}_{i=1} P ( x_i \vert c ) \\]

此时的分类器称为朴素贝叶斯分类器。

贝叶斯分类器与决策树、神经网络、支持向量机等“判别式模型”的不同之处就在于它是一种间接判断模式，被称为“生成式模型”。

下面尝试使用贝叶斯分类器对文本进行分类过滤。

### 文本向量化 {#text2vec}

将文本转换为向量，也就是对每句话进行分词(这里以英文为例，中文可使用结巴[分词](https://github.com/fxsjy/jieba){:target="_blank"})，每个词作为一个元素，组成列表。关于文本处理，可查看[这里](http://about.uuspider.com/2016/04/08/pyre.html){:target="_blank"}。

    sentence = 'today is your last day to purchase holiday votives before anyone else'
    s_vec = [x.strip() for x in sentence.split()]
    words_li = [['today', 'is', 'your', 'last', 'day', 'to', 'purchase', 'holiday', 'votives', 'before', 'anyone', 'else'],
               ['Police', 'have', 'been', 'unable', 'to', 'tell', 'the', 'age', 'or', 'gender', 'of', 'the', 'deceased', 'person', 'because', 'the', 'remains', 'were', 'so', 'charred'],
               ['We', 'will', 'be', 'keeping', 'this', 'part', 'of', 'the', 'page', 'updated', 'with', 'testimonials', 'from', 'people', 'who', 'have', 'been', 'successful', 'using', 'xx', 'in', 'China'],
               ['Tis', 'the', 'season', 'to', 'buy', '2', 'dozen', 'votives', 'for', 'only', '$20', 'and', 'get', 'one', '6-pack', 'of', 'votives', 'free'],
               ['Anytime', 'you', 'enter', 'into', 'a', 'credit', 'contract', 'the', 'lender', 'sends', 'that', 'to', 'the', 'federal', 'bank', 'database'],
               ['Shop', 'service', 'pride', 'items', 'for', 'every', 'branch', 'of', 'service']]

作为训练集，对每一句话的单词列表都进行人工标记，以确定其所属类别：

    class_vec = [1,0,0,1,0,1]

其中1表示来自于spam，0是正常文本。

生成一个单词的集合，并转换为列表：

    words_set = set([])
    for doc in words_li:
        words_set = words_set | set(doc)
    all_words_li = list(words_set)

将每一个单词列表都转成向量：

    words_vec = [0]*len(all_words_li)
    for word in words_li[2]:
        if word in all_words_li:
            words_vec[all_words_li.index(word)] = 1

最后将以单词向量为元素，整个训练集就转换为一个嵌套列表(矩阵)，记为words_matrix。

### 分类器训练函数 {#bayes_train}

    import numpy as np
    def train_naive_bayes(matrix_t,category_t):
        num_instance = len(matrix_t)
        num_feature = len(matrix_t[0])
        p_to_filter = sum(category_t)/float(num_instance)
        p0_num = np.zeros(num_feature); p1_num = np.zeros(num_feature)
        p0_sum = 0.0; p1_sum = 0.0
        for i in range(num_instance):
            if category_t[i] == 1:
                p1_num += matrix_t[i]
                p1_sum += sum(matrix_t[i])
            else:
                p0_num += matrix_t[i]
                p0_sum += sum(matrix_t[i])
        p1_vec = log(p1_num/p1_sum)
        p0_vec = log(p0_num/p0_sum)
        return p0_vec, p1_vec, p_to_filter

使用训练集进行训练：

    p0vec, p1vec, p2filter = train_naive_bayes(words_matrix,class_vec)

### 朴素贝叶斯分类器 {#bayes_main}

    def classify_by_native_bayes(vec2classify,p0vec,p1vec,p_class1):
        p1 = sum(vec2classify * p1vec) + log(p_class1)
        p0 = sum(vec2classify * p0vec) + log(1.0-p_class1)
        if p1 > p0:
            return 1
        else:
            return 0

对新的输入文本进行分类，如：

    new_words_li = ['no', 'news', 'is', 'good', 'news']

将其转为向量new_words_vec后：

    print classify_by_native_bayes(new_words_vec,p0vec,p1vec,p2filter)

返回1表示为spam，返回0为正常文本。

**[[TOP](#top)]**

***
