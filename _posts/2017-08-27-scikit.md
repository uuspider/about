---
layout: post
title: Data science & scikit-learn
---

<script type="text/javascript" src="{{site.baseurl}}/MathJax-2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Data science & scikit-learn

参考：[Machine Learning Map][ref1]{:target="_blank"}, [urllib2 - extensible library for opening URLs][ref2]{:target="_blank"}

[ref1]:http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
[ref2]:https://docs.python.org/2/library/urllib2.html

<h2 id="top"></h2>

***

*   [数据导入](#input)
*   [距离计算](#distance): [Minkowski](#minkowski)\|[Euclidean](#euclidean)\|[Manhattan](#manhattan)\|[cosine](#cosine)\|[Jaccard](#jaccard)
*   [k近邻](#knn)
*   [贝叶斯分类器](#proxy)
*   [决策树](#timeout)
*   [Logistic回归](#error)
*   [支持向量机](#error)
*   [k-means聚类](#error)
*   [Bagging](#error)
*   [AdaBoost](#error)
*   [随机森林](#error)
*   [超随机树](#error)
*   [旋转森林](#error)


***

## 数据导入 {#input}

数据导入是数据处理、机器学习的第一步。首先对数据进行清洗并存储为csv格式，之后导入数据集为嵌套列表：

    import numpy as np
    dataset = np.loadtxt('x.csv', delimiter=",")

numpy的更多用法，请查看[这里](http://about.uuspider.com/2015/08/29/numpy.html)。

## 距离表征 {#distance}

### Minkowski距离 {#minkowski}

Minkowski距离的计算公式为：

\\[ d ( {\bf x}, {\bf y} ) = ( \sum_{i=1}^{n} \vert x_i - y_i \vert ^p)^\frac{1}{p} \\]


    def minkowski_distance(x,y,power):
        if len(x) == len(y):
            return np.power(np.sum(np.power((x-y),power)),(1/(1.0*power)))
        else:
            print "Input should be of equal length"
        return None

### Euclidean距离 {#euclidean}

令Minkowski距离公式中的p=2，得到Euclidean距离公式：

\\[ d ( {\bf x}, {\bf y} ) = ( \sum_{i=1}^{n} \vert x_i - y_i \vert ^2)^\frac{1}{2} \\]


    def euclidean_distance(x,y):
        return minkowski_distance(x,y,2)

### Manhattan距离 {#manhattan}

令Minkowski距离公式中的p=1，得到Manhattan距离公式：

\\[ d ( {\bf x}, {\bf y} ) = \sum_{i=1}^{n} \vert x_i - y_i \vert \\]


    def euclidean_distance(x,y):
        return minkowski_distance(x,y,1)

### cosine距离 {#cosine}

将Euclidean距离加1，然后取倒数，就可以得到一个介于0到1之间的值，可以用来表示两个向量相似的程度，1表示完全相同。

与此类似，两个向量夹角的余弦值可以更准确地判断这两个向量的相似程度，它反映的是这两个向量与某一条直线的拟合程度，因此也被称为Pearson相关系数，该系数介于-1到1之间，1表示完全正相关，0表示无关，-1表示完全负相关。Pearson相关系数常常用于文本挖掘，词是轴，文档为向量，Pearson系数就代表了这两个文档之间的相似度。

\\[ r ( {\bf x}, {\bf y} ) = \frac{\sum xy - \frac{\sum x \sum y}{n}}{\sqrt{(\sum x^2 - \frac{(\sum x)^2}{n})(\sum y^2 - \frac{(\sum y)^2}{n})}}\\]


    def cosine_distance(x,y):
        if len(x) == len(y):
            return np.dot(x,y)/np.sqrt(np.dot(x,x)*np.dot(y,y))
        else:
            print "Input should be of equal length"
        return None

### Jaccard距离 {#jaccard}

Jaccard距离属于非欧空间距离。两个集合的交集与并集的比值称为Jaccard系数，Jaccard系数可用于衡量聚类算法的性能。1减去Jaccard系数就是Jaccard距离。


    def jaccard_distance(x,y):
        set_x = set(x)
        set_y = set(y)
        return 1-len(set_x.intersection(set_y)) / len(set_x.union(set_y))
