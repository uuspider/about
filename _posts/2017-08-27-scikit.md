---
layout: post
title: Data science & scikit-learn
---

<script type="text/javascript" src="{{site.baseurl}}/MathJax-2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Data science & scikit-learn

参考：[Machine Learning Map][ref1]{:target="_blank"}, [urllib2 - extensible library for opening URLs][ref2]{:target="_blank"}

[ref1]:http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
[ref2]:https://docs.python.org/2/library/urllib2.html

<h2 id="top"></h2>

***

*   [数据导入](#input)
*   [k近邻](#knn)
    *   [距离表征](#distance): [Minkowski](#minkowski) \| [Euclidean](#euclidean) \| [Manhattan](#manhattan) \| [cosine](#cosine) \| [Jaccard](#jaccard)
    *   [数据分割](#split)
    *   [数据归一化](#norm)
    *   [查找最近邻](#knn_main)
    *   [模型性能表征](#performance)
*   [贝叶斯分类器](#bayes)
    *   [贝叶斯公式](#bayes_eq)
    *   [文本向量化](#text2vec)
    *   [高斯朴素贝叶斯](#bayes_g)
    *   [多项式朴素贝叶斯](#bayes_m)
    *   [伯努利朴素贝叶斯](#bayes_b)
*   [决策树](#decision_tree)
    *   [不纯度表征](#impurity): [信息增益](#entropy) \| [增益率](#gainratio) \| [基尼指数](#gini)
    *   [分类决策树](#decision_tree_c)
    *   [决策树的可视化](#decision_tree_graph)
    *   [模型的保存](#save)
    *   [回归决策树](#decision_tree_r)
*   [Logistic回归](#logistic)
    *   [Sigmoid函数](#sigmoid)
    *   [范数](#norm)
    *   [梯度上升算法](#loss)
    *   [多元分类](#multinomial)
    *   [Logistic回归](#logistic_main)
*   [支持向量机](#error)
*   [k-means聚类](#error)
*   [Bagging](#error)
*   [AdaBoost](#error)
*   [随机森林](#error)
*   [超随机树](#error)
*   [旋转森林](#error)


***

## 数据导入 {#input}

数据导入是数据处理、机器学习的第一步。首先对数据进行清洗并存储为csv格式，之后导入数据集为嵌套列表：

    import numpy as np
    dataset = np.loadtxt('x.csv', delimiter=",")

numpy的更多用法，请查看[这里](http://about.uuspider.com/2015/08/29/numpy.html)。

**[[TOP](#top)]**

***

## k近邻 {#knn}

### 距离表征 {#distance}

### Minkowski距离 {#minkowski}

Minkowski距离也被称为Lp距离，其计算公式为：

\\[ d ( {\bf x}, {\bf y} ) = ( \sum_{i=1}^{n} \vert x_i - y_i \vert ^p)^\frac{1}{p} \\]


    def minkowski_distance(x,y,power):
        if len(x) == len(y):
            return np.power(np.sum(np.power((x-y),power)),(1/(1.0*power)))
        else:
            print "Input should be of equal length"
        return None

这里p\\( p \geqslant 1 \\)，当\\( p \to \infty \\)时，得到Chebyshev距离：

\\[ d ( {\bf x}, {\bf y} ) = ( max \vert x_i - y_i \vert ) \\]

实际上，当\\( p \to 0 \\)时，得到的是Hamming距离，Hamming距离最早用于数据传输差错控制编码，它表示两个（相同长度）字对应位不同不同的数量，如：

1011101 与 1001001 之间的汉明距离是 2。

2143896 与 2233796 之间的汉明距离是 3。

"toned" 与 "roses" 之间的汉明距离是 3。

对两个字符串进行异或运算，并统计结果为1的个数，这个数就是Hamming距离。

### Euclidean距离 {#euclidean}

令Minkowski距离公式中的p=2，得到Euclidean距离公式：

\\[ d ( {\bf x}, {\bf y} ) = ( \sum_{i=1}^{n} \vert x_i - y_i \vert ^2)^\frac{1}{2} \\]


    def euclidean_distance(x,y):
        return minkowski_distance(x,y,2)

### Manhattan距离 {#manhattan}

令Minkowski距离公式中的p=1，得到Manhattan距离公式：

\\[ d ( {\bf x}, {\bf y} ) = \sum_{i=1}^{n} \vert x_i - y_i \vert \\]


    def euclidean_distance(x,y):
        return minkowski_distance(x,y,1)

### cosine距离 {#cosine}

将Euclidean距离加1，然后取倒数，就可以得到一个介于0到1之间的值，可以用来表示两个向量相似的程度，1表示完全相同。

与此类似，两个向量夹角的余弦值可以更准确地判断这两个向量的相似程度，它反映的是这两个向量与某一条直线的拟合程度，因此也被称为Pearson相关系数，该系数介于-1到1之间，1表示完全正相关，0表示无关，-1表示完全负相关。Pearson相关系数常常用于文本挖掘，词是轴，文档为向量，Pearson系数就代表了这两个文档之间的相似度。

\\[ r ( {\bf x}, {\bf y} ) = \frac{\sum xy - \frac{\sum x \sum y}{n}}{\sqrt{(\sum x^2 - \frac{(\sum x)^2}{n})(\sum y^2 - \frac{(\sum y)^2}{n})}}\\]


    def cosine_distance(x,y):
        if len(x) == len(y):
            return np.dot(x,y)/np.sqrt(np.dot(x,x)*np.dot(y,y))
        else:
            print "Input should be of equal length"
        return None

对于矩阵a，`np.corrcoef(a)`和`np.corrcoef(a,rowvar=0)`可以计算各行和各列之间的相关系数，如：

    >>> a = [[1, 2, 3, 4, 5],
            [8, 3, 4, 2, 6],
            [9, 5, 7, 2, 5]]
    >>> np.corrcoef(a)
    array([[ 1.        , -0.32826608, -0.66697297],
          [-0.32826608,  1.        ,  0.80412382],
          [-0.66697297,  0.80412382,  1.        ]])

### Jaccard距离 {#jaccard}

Jaccard距离属于非欧空间距离。两个集合的交集与并集的比值称为Jaccard系数，Jaccard系数可用于衡量聚类算法的性能。1减去Jaccard系数就是Jaccard距离。


    def jaccard_distance(x,y):
        set_x = set(x)
        set_y = set(y)
        return 1-len(set_x.intersection(set_y)) / len(set_x.union(set_y))

### 数据分割 {#split}

kNN是一种分类算法，其处理的数据集格式通常是这样的：

1. 每一行是一个实例(或记录、样本等)；

2. 每一列表示一个特征(或属性等)；

3. 最后一列是标签(或类别、标记等)。

对于有监督的kNN算法，需要将导入的数据合理分割为训练集和测试集，为使训练集和测试集中的数据随机分布，首先将数据打乱：

    >>> np.random.shuffle(dataset)
    >>> train_size = 0.8
    >>> test_size = 1-train_size
    >>> from sklearn.cross_validation import train_test_split
    >>> train, test = train_test_split(dataset,test_size=test_size)

### 数据的归一化 {#norm}

将训练集中的特征矩阵和标签向量分开：

    for indx in range(len(train[0])):
        x = train[:,:-1]
        y = train[:,-1]

对于x中不同的特征、属性，取值可能相差很大，如年龄的取值一般为0到100，而行驶里程有可能在0到几万甚至几十万之间取值，这会影响距离计算结果的合理性。因此，有时候，需要对数据集进行归一化处理，也就是将所有特征、属性的取值范围转化在0到1之间。

    from sklearn import preprocessing
    x = preprocessing.scale(x)

### 查找k近邻 {#knn_main}

指定k值创建kNN分类器，并开始训练：

    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier(n_neighbors=10)
    knn.fit(x,y)

如果数据量非常大，可以使用`partial_fit`将训练集分成多次载入内存。

训练完成后将返回knn对象信息：

    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
       metric_params=None, n_jobs=1, n_neighbors=10, p=2,
       weights='uniform')

这些参数都是可以指定的，如`algorithm`包含`ball_tree`， `kd_tree`， `brute`以及`auto`，`leaf_size`是一个整数，用于指定`ball_tree`、 `kd_tree`叶节点的数，`metric`用于指定距离度量方式，`n_jobs`指定并行计算线程数量，-1表示CPU的内核数，`n_neighbors`指定k值，p指定Minkowski距离中的p值，`weights`是权重，`uniform`表示等比重，`distance`表示按距离反比等等。

这时候可以查看当给定x矩阵时，模型预测的类别：

    y_predicted = knn.predict(x)

计算预测结果的概率和概率的对数：

    knn.predict_proba(x)
    knn.predict_log_proba(x)

计算距离测试样本最近的10个点，返回的是这些实例所在的行序号：

    knn.kneighbors(x,10,False)

第三个参数是`return_distance`，指定是否返回具体的距离值。

### 模型性能表征 {#performance}

对照模型预测分类和真实分类，就可以评价该模型的性能：

    from sklearn import metrics
    print metrics.classification_report(y,y_predicted)

结果是类似这样的报告形式：

            precision    recall  f1-score   support

    class 0       0.50      1.00      0.67         1
    class 1       0.00      0.00      0.00         1
    class 2       1.00      0.67      0.80         3

    avg / total       0.70      0.60      0.61         5

查看模型的精度(Accuracy)：

    knn.score(x,y,sample_weight=None)

查看模型的混淆矩阵(Confusion Matrix)：

    print metrics.confusion_matrix(y,y_predicted)

混淆矩阵含有丰富的信息：

||Positive|Negative||
|Positive|TP|FP|P'|
|Negative|FN|TN|N'|
||P|N||

可以计算准确率(查准率，Precision)：

\\[ P = \frac{TP}{TP+FP}=\frac{TP}{P'} \\]

    print metrics.precision_score(y,y_predicted)

召回率(查全率，Recall)：

\\[ R = \frac{TP}{TP+FN}=\frac{TP}{P} \\]

    print metrics.recall_score(y,y_predicted)

精度(准确度，Accuracy)：

\\[ A = \frac{TP+TN}{P+N} \\]

    print metrics.accuracy_score(y,y_predicted)

错误率(Error Rate)：

\\[ E = \frac{FP+FN}{P+N} \\]

F1值：

\\[ F1 Score = \frac{2PR}{P+R} \\]

    print metrics.f1_score(y,y_predicted)

此外，metrics模块还有`roc_curve`和`auc`等方法。

**[[TOP](#top)]**

***

## 贝叶斯分类器 {#bayes}

### 贝叶斯公式 {#bayes_eq}

贝叶斯分类器的理论基础是贝叶斯公式：

\\[ P ( c \vert {\bf x } ) = \frac{ P ( {\bf x } \vert c ) P ( c ) }{ P ( {\bf x } ) }\\]

这里的P(c)为先验概率，P(x\|c)是样本x相对于类标记c的类条件概率，P(x)是用于归一化的因子，对于给定的样本x，P(x)与类标记无关，因此，估计P(c\|x)的问题就转化为计算P(x\|c)P(c)。贝叶斯分类器与决策树、神经网络、支持向量机等“判别式模型”的不同之处就在于它是一种间接判断模式，被称为“生成式模型”。

如果x中各实例的特征、属性之间相互独立，即每个特征、属性**独立**影响分类结果，则有

\\[ P ( {\bf x } \vert c ) = \prod ^{t}_{i=1} P ( x_i \vert c ) \\]

此时的分类器称为朴素贝叶斯分类器。

对\\(P( x_i \vert c)\\)的不同假设，朴素贝叶斯分类器可分为高斯朴素贝叶斯(Gaussian Naive Bayes)，多项式朴素贝叶斯(Multinomial Naive Bayes)和伯努利朴素贝叶斯(Bernoulli Naive Bayes)。

下面尝试使用贝叶斯分类器对文本进行分类过滤。

### 文本向量化 {#text2vec}

对文本进行分词(这里以英文为例，中文可使用[结巴分词](https://github.com/fxsjy/jieba){:target="_blank"})，每个词作为一个元素，组成列表。关于文本处理，可查看[这里](http://about.uuspider.com/2016/04/08/pyre.html){:target="_blank"}。

    sentence = 'today is your last day'
    s_vec = [x.strip() for x in sentence.split()]
    words_li = [['today', 'is', 'your', 'last', 'day'],
               ['Police', 'have', 'been', 'unable', 'to', 'tell', 'the', 'age'],
               ['We', 'will', 'be', 'keeping', 'this', 'part', 'updated'],
               ['Tis', 'the', 'season', 'to', 'buy', 'votives'],
               ['the', 'lender', 'sends', 'that', 'to', 'the', 'federal', 'bank', 'database'],
               ['Shop', 'service', 'pride', 'items', 'for', 'every', 'branch', 'of', 'service']]

作为训练集，对每一句话的单词列表都进行人工标记，以确定其所属类别：

    class_vec = [1,0,0,1,0,1]

其中1表示来自于spam，0是正常文本。

生成一个单词的集合，并转换为列表：

    words_set = set([])
    for doc in words_li:
        words_set = words_set | set(doc)
    all_words_li = list(words_set)

将每一个单词列表都转成向量：

    words_matrix = []
    for i in range(len(words_li)):
        words_vec = [0]*len(all_words_li)
        for word in words_li[i]:
            if word in all_words_li:
                words_vec[all_words_li.index(word)] = 1
        words_matrix.append(words_vec)

最后将以单词向量为元素，整个训练集就转换为一个嵌套列表(矩阵)，记为words_matrix。

### 高斯朴素贝叶斯 {#bayes_g}

高斯朴素贝叶斯假设\\(P( x_i \vert c)\\)为正态分布：

\\[ P ( x_i \vert c ) = \frac{1}{\sqrt{2 \pi \sigma ^2 _c}} \exp(-\frac{(x_i - \mu _c)^2}{2 \sigma ^2 _c})\\]

其中\\(\mu _c\\)和\\(\sigma ^2 _c\\)需要从训练集中估计。

    from sklearn.naive_bayes import GaussianNB
    gnb = GaussianNB()
    gnb.fit(words_matrix,class_vec)

对新的输入文本进行分类，如：

    new_words_li = ['no', 'news', 'is', 'good', 'news']

将其转为向量new_words_vec，再转为矩阵new_words_matrix后：

    gnb.predict(new_words_matrix)

### 多项式朴素贝叶斯 {#bayes_m}

多项式朴素贝叶斯假设\\(P( x_i \vert c)\\)为多项式分布：

\\[ P ( x_i \vert c ) = \frac{ x_i + \lambda }{ m + n \lambda }\\]

    from sklearn.naive_bayes import MultinomialNB
    mnb = MultinomialNB()
    mnb.fit(words_matrix,class_vec)

`MultinomialNB()`有3个参数，`alpha`对应于公式中的\\(\lambda\\)，取值通常为1，`fit_prior`是一个布尔值，表示是否考虑先验概率，当取值为True时，可设定`class_prior`指定先验概率。

### 伯努利朴素贝叶斯 {#bayes_b}

伯努利朴素贝叶斯假设\\(P( x_i \vert c)\\)为二元伯努利分布：

\\[ P ( x_i \vert c ) = P ( i \vert c ) x_i + ( 1 - P ( i \vert c ))( 1 - x_i )\\]

    from sklearn.naive_bayes import BernoulliNB
    bnb = BernoulliNB()
    bnb.fit(words_matrix,class_vec)

**[[TOP](#top)]**

***

## 决策树 {#decision_tree}

### 不纯度表征 {#impurity}

### 信息增益 {#entropy}

在给定的样本集合中，每个实例可能属于不同的类别，衡量集合纯度最常用的指标就是熵(entropy)，其计算公式为：

\\[ Ent ( X ) = - \Sigma ^n _{i=1} P ( x _i ) \log_2 P ( x _i ) \\]

熵越小，表示集合的纯度越高，如果集合所有元素属于同一类，则：

\\[ Ent ( X ) = - \Sigma ^n _{i=1} 1 \log_2 1 = 0 \\]

信息增益可用于表征使用特征(属性)a划分集合时引起的纯度提升，计算公式为：

\\[ Gain( X, a ) = Ent ( X ) - \Sigma ^v _{ \upsilon = 1 } \frac{ \vert X^{\upsilon} \vert }{ \vert X \vert } Ent( X^{\upsilon} ) \\]

v是特征(属性)a所有可能取值的数目，每一个取值产生一个类。

### 增益率 {#gainratio}

增益率(gain ratio)是另外一种衡量集合纯度变化的指标，其计算公式为：

\\[ Gain\\_Ratio ( X, a ) = \frac{ Gain( X, a )}{ IV(a) } \\]

其中：

\\[ IV(a) = - \Sigma ^v _{ \upsilon = 1 } \frac{ \vert X^{\upsilon} \vert }{ \vert X \vert } \log_2 \frac{ \vert X^{\upsilon} \vert }{ \vert X \vert } \\]

是特征(属性)a的固有熵值(intrinsic value)，该值越大，表示特征(属性)a的可能取值数目越多。

### 基尼指数 {#gini}

基尼值也可用于表征数据集的纯度：

\\[ Gini(X) = \Sigma ^{\vert y \vert} _{i=1} \Sigma _{j \neq i} P ( x _i ) P ( x _j ) = 1 - \Sigma ^{\vert y \vert} _{i=1} (P ( x _i ))^2 \\]

Gini(X)反映了从数据集中随机抽取两个实例，其类别不一致的概率，Gini(X)越小，数据集的纯度越高。

特征(属性)a的基尼指数的计算公式为：

\\[ Gini\\_Index (X,a) = \Sigma ^v _{ \upsilon =1} \frac{ \vert X^{\upsilon} \vert }{ \vert X \vert } Gini( X^{\upsilon} ) \\]

采用信息增益、增益率和基尼指数的决策树分别为ID3、C4.5和CART算法。下面即将讲述的`DecisionTreeClassifier`使用的是优化的CART，设置`criterion`参数为`entropy`(信息增益)，可近似为ID3算法，C4.5基于信息增益率，scikit-learn不能直接实现。

注：ID3中的ID为Iterative Dichotomiser，迭代二分器，CART为Classification and Regression Tree，分类和回归树。

有了上述公式，信息增益、增益率和基尼指数的python实现并不难，对于决策树而言，其可视化更值得关注和研究。

### 分类决策树 {#decision_tree_c}

分类决策树所采用的数据集格式与kNN相同，训练集中最后一列为标签，分离特征矩阵和标签向量：

    for indx in range(len(train[0])):
        x = train[:,:-1]
        y = train[:,-1]

采用默认的CART算法开始训练：

    from sklearn import metrics
    from sklearn.tree import DecisionTreeClassifier
    dtc = DecisionTreeClassifier()
    dtc.fit(x,y)

此时将返回dtc的对象信息：

    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
        max_features=None, max_leaf_nodes=None,
        min_impurity_decrease=0.0, min_impurity_split=None,
        min_samples_leaf=1, min_samples_split=2,
        min_weight_fraction_leaf=0.0, presort=False, random_state=None,
        splitter='best')

可以看到，可指定的参数非常丰富。

`class_weight`， 指定样本各类别的权重，防止训练集某些类别的样本过多，导致决策树过于偏向这些类别。默认为'None'；可指定为'balanced'，设定算法自动调整权重；也可以分别指定各个样本的权重，格式为{0:0.9,1:0.1}。

`criterion`， 可设定为`gini`或`entropy`，分别对应CART和ID3算法，默认`gini`。

`max_depth`， 决策树的最大深度，默认为'None'，表示不限制深度，如果实例量或特征数目很多，可设定该值，通常在10-100之间。

`max_features`， 默认为'None'，表示考虑所有特征；如果为'log2'，表示最多考虑\\(log_2N\\)个特征(N为数据集特征总数目)；如果是'sqrt'或'auto'，表示最多考虑\\(\sqrt{N}\\)个特征；如果是整数，表示考虑的特征数目；如果是浮点数，表示考虑N乘以该浮点数并取整后的特征数目。如果样本特征数目不多，默认即可；如果特征数目非常多，可设定该参数以控制决策树建模时间。

`max_leaf_nodes`， 默认为'None'，表示不限制最大的叶子节点数。如果特征数目很多，可以通过交叉验证定一个数值，防止过拟合。

`min_impurity_decrease`， 浮点数，默认为0，当不纯度的降幅小于该值时终止分类。

`min_impurity_split`， 默认为2，表示节点样本数少于2时，不会继续尝试选择最优特征进行分类，如果样本量非常大，如10万样本，可考虑设定为10。注：v0.21可能会取消该参数，以`min_impurity_decrease`替代。

`min_samples_leaf`， 默认为1，表示叶子节点最少的样本数目，可设定为其它整数；如果是浮点数，表示最少样本数占样本总数的百分比。样本量非常大时，可设定该值以提高效率。

`min_samples_split`

`min_weight_fraction_leaf`， 表示叶子节点所有样本权重和的最小值，如果小于该值，会和兄弟节点一起被剪枝；默认是0，表示不考虑权重问题。当样本有较多缺失值或样本分布类别偏差很大时，可考虑设定该值。

`presort`， 布尔值，默认为'False'，表示不对数据排序，样本量大时，设定为'True'会拖慢建模速度。

`splitter`， 默认为'best'，适合样本量不大的情况，如果样本量很大，可设定为'random'。

更多参数可查看官方手册。决策树对象还有一些属性和方法，可通过help(dtc)查看。

这时候可以查看当给定x矩阵时，模型预测的标签：

    y_predicted = dtc.predict(x)

### 决策树的可视化 {#decision_tree_graph}

决策树经过可视化处理后，其结构会更加清晰，便于理解，决策树可视化一般采用graphviz，需要安装graphviz及相关python模块。

    $ sudo apt install graphviz
    $ sudo apt install graphviz-dev  #这是和python库链接用的
    $ pip install pygraphviz
    $ pip install pydotplus  #用于处理dot文件

决策树对象创建完成后可导出为dot文件：

    from sklearn import tree
    with open('out.dot','w') as f:
        f = tree.export_graphviz(dtc,out_file=f)

dot文件是AutoCAD的图纸文件格式，很多绘图软件都支持，常用于共享数据。在shell中可以快速转为jpg, png, pdf等格式：

    $ dot -Tpng out.dot -o out.png

也可以使用pydotplus模块进行处理：

    import pydotplus
    dot_data = tree.export_graphviz(dtc,out_file=None)
    graph = pydotplus.graph_from_dot_data(dot_data)
    graph.write_pdf("out.pdf")

### 模型的保存 {#save}

决策树可以使用`sklearn`的`joblib`模块保存到硬盘或从硬盘导入：

    from sklearn.externals import joblib
    joblib.dump(dtc,'dtc.model')
    dtc = joblib.load('dtc.model')

注：`joblib`也适用于其他模型、对象的保存和导入。

### 回归决策树 {#decision_tree_r}

决策树可以用于回归，此时x为自变量(矩阵)，y是因变量(向量)，当输入x时返回y，回归就是拟合，目的是寻找y随x的变化规律：

    from sklearn import metrics
    from sklearn.tree import DecisionTreeRegressor
    dtr = DecisionTreeRegressor()
    dtr.fit(x,y)

其参数与`DecisionTreeClassifier`类似，其中，`criterion`可选`mse`或`mae`，前者为均方差，后者是和均值之差的绝对值之和，默认为`mse`。

完成模型构建后，可以根据给定x矩阵，预测相应的y向量：

    y_predicted = dtr.predict(x)

分类处理的数据集中，y只能是离散值、标签或者'True/False'二元值，而回归处理的数据集中y可以是任意实数。

**[[TOP](#top)]**

***

## Logistic回归 {#logistic}

### Sigmoid函数 {#sigmoid}

对数几率回归虽然带有“回归”两个字，但实际上更多用于分类，是用回归的思想解决分类问题。

回归就是根据一系列已知的x和y，去推测函数方程的形式，并计算每一项的系数，拟合出来的直线或曲线应该是连续的，而分类显然不是连续的。

这就要借助于Sigmoid函数(S形函数)，最常见的是Logistic函数(对数几率函数)，Logistic函数是连续函数，但近似于阶跃函数：

\\[ \sigma (z) = \frac{1}{1+e^{-z}} \\]

该函数值取值在0到1之间，z=0时，\\( \sigma = 0.5\\)，z减小时，\\( \sigma\\)快速逼近0，z增大时，\\(\sigma\\)快速逼近1。

把回归方程作为z带入Logistic函数中，可以得到一个0到1之间的数值，当该值小于0.5时实例被归入0类，否则归入1类，这样就实现了分类的功能。

### 范数 {#norm}

范数\\(L_p\\)是具有“长度”概念的函数，和[距离表征](#distance)有关。

0范数\\(L_0\\)表示向量到原点的Hamming距离，1范数\\(L_1\\)是向量到原点的Manhattan距离，2范数\\(L_2\\)是Euclidean距离，无穷范数\\(L_{\infty}\\)是Chebyshev距离。

下面我们将用到\\(L_1\\)和\\(L_2\\)，其计算公式为：

\\[ L_1 = \sum_{i=1}^{n} \vert x_i \vert \\]

\\[ L_2 = ( \sum_{i=1}^{n} \vert x_i \vert ^2)^\frac{1}{2} \\]

\\(L_1\\)可使矩阵稀疏化，当需要减少样本的特征数目时，可考虑使用\\(L_1\\)。

### 梯度上升算法 {#loss}

已知实例的特征矩阵x，标签向量y，确定回归系数\\(\omega\\)。引入误差e和步长\\(\alpha\\)，最佳的\\(\omega\\)需要使\\(e^Te\\)最小化，为推导方便，引入一个\\(-\frac{1}{2}\\)作为系数，也就是使\\(-\frac{1}{2}e^Te\\)最大化，令：

\\[ f(\omega) = -\frac{1}{2}e^Te = -\frac{1}{2}(x \omega -y)^T(x \omega -y) \\]

这里\\(f(\omega)\\)称为损失函数或代价函数，展开后为

\\[ f(\omega) = -\frac{1}{2}( \omega ^T x^T x \omega - \omega ^T x ^T y - y^T x \omega + y^Ty) \\]

对\\(\omega\\)求导(矩阵求导可参考相关资料)：

\\[ \frac{\partial f (\omega)}{\partial \omega} = x^T -x^T x \omega = x^T(y - \omega x) = - x^T e \\]

可以得到梯度上升算法的迭代式：

\\[ \omega := \omega - \frac{\partial f (\omega)}{\partial \omega} = \omega + x^T e \\]

引入步长\\(\alpha\\)：

\\[ \omega := \omega + \alpha x^T e \\]

这里以梯度上升算法阐述了回归系数的确定方法，sklearn的`LogisticRegression`内置了多种上升或下降算法用于确定迭代方向，如`liblinear`采用坐标下降算法，`lbfgs`和`newton-cg`采用拟牛顿法，`sag`采用随机平均梯度下降算法，其中，`liblinear`多用于处理较小的数据集，`sag`多用于海量数据集。

牛顿法是回归算法中最常用的一种算法，但是由于需要计算二阶导数，生成并存储逆Hessian矩阵，其运算量和需要的内存非常大，因此在工程上用的并不多。BFGS(Broyden-Fletcher-Goldfarb-Shanno)是一种拟牛顿法，只需要计算近似逆Hessian矩阵，但所需内存和计算量仍然很大，sklearn中的`lbfgs`是Limited-memory BFGS算法，`newton-cg`是牛顿法的共轭梯度(Conjugate Gradient)近似算法，都属于拟牛顿法，但不需要计算二阶导数，其收敛速度比梯度下降算法快，运算速度比牛顿法快，需要的内存空间比牛顿法小，适用于处理实例数量较大的数据集。

### 多元分类 {#multinomial}

从Sigmoid函数的特点可以看到，Logistic回归只能处理二元分类，实际上，也可以处理多元分类，主要有两种方法：`ovr`即'one vs rest'，把一种类别与其它类别分开，再进一步从其它类别中分出一个类别，经过多次循环就达到了多元分类的目的；`multinomial`即‘mvm，many vs many’，每次取两个类进行回归，如果共有T类，共需要进行T(T-1)/2次分类，而'ovr'只需要T次，'mvm'算法速度慢一些，但精度更高。

### Logistic回归 {#logistic_main}

    from sklearn.linear_model import LogisticRegression
    lr = LogisticRegression()
    lr.fit(x,y)

完成建模后将返回模型的详细参数信息：

    LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0,
        fit_intercept=True, intercept_scaling=1, class_weight=None,
        random_state=None, solver='liblinear', max_iter=100,
        multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)

`tol`， 迭代至误差小于该指定值时终止。

`max_iter`， 最大迭代次数。

`C`， 正则化参数\\(\lambda\\)的倒数，只能是正数，越小正则越强。

`solver`， 指定下降算法，包括`liblinear`，`lbfgs`， `newton-cg`，`sag`。

`multi_class`， 指定多元分类的实现方法，包括`ovr`和`multinomial`。

`penalty`， 指定惩罚基准(正则化参数)，包括`l1`和`l2`。

Logistic回归的调参比较复杂，首先可以根据数据量的大小选择算法，如数据量小可优先选择`liblinear`，数据量非常大可选择`sag`，数据量较大选`lbfgs`或`newton-cg`；`liblinear`不支持'mvm'，如果需要精确分类可考虑其它算法；如果`l2`过拟合，或数据集的特征很多，如某些生物样本，特征数目多达几千，而实例数目可能只有几百，这时候需要选`l1`，但`l1`只支持`liblinear`。

**[[TOP](#top)]**

***
