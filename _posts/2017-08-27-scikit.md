---
layout: post
title: Data science & scikit-learn
---

<script type="text/javascript" src="{{site.baseurl}}/MathJax-2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Data science & scikit-learn

参考：[Machine Learning Map][ref1]{:target="_blank"}, [urllib2 - extensible library for opening URLs][ref2]{:target="_blank"}

[ref1]:http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
[ref2]:https://docs.python.org/2/library/urllib2.html

<h2 id="top"></h2>

***

*   [数据导入](#input)
*   [k近邻](#knn)
    *   [距离表征](#distance):[Minkowski](#minkowski)\|[Euclidean](#euclidean)\|[Manhattan](#manhattan)\|[cosine](#cosine)\|[Jaccard](#jaccard)
    *   [数据归一化](#norm)
    *   [数据分割](#split)
    *   [查找最近邻](#knn_main)
*   [贝叶斯分类器](#bayes)
    *   [生成式模型](#generative)
    *   [文本向量化](#text2vec)
    *   [高斯朴素贝叶斯](#bayes_g)
    *   [多项式朴素贝叶斯](#bayes_m)
    *   [伯努利朴素贝叶斯](#bayes_b)
*   [决策树](#decision_tree)
    *   [信息增益、增益率和基尼指数](#entropy)
*   [Logistic回归](#error)
*   [支持向量机](#error)
*   [k-means聚类](#error)
*   [Bagging](#error)
*   [AdaBoost](#error)
*   [随机森林](#error)
*   [超随机树](#error)
*   [旋转森林](#error)


***

## 数据导入 {#input}

数据导入是数据处理、机器学习的第一步。首先对数据进行清洗并存储为csv格式，之后导入数据集为嵌套列表：

    import numpy as np
    dataset = np.loadtxt('x.csv', delimiter=",")

numpy的更多用法，请查看[这里](http://about.uuspider.com/2015/08/29/numpy.html)。

**[[TOP](#top)]**

***

## k近邻 {#knn}

### 距离表征 {#distance}

### Minkowski距离 {#minkowski}

Minkowski距离也被称为Lp距离，其计算公式为：

\\[ d ( {\bf x}, {\bf y} ) = ( \sum_{i=1}^{n} \vert x_i - y_i \vert ^p)^\frac{1}{p} \\]


    def minkowski_distance(x,y,power):
        if len(x) == len(y):
            return np.power(np.sum(np.power((x-y),power)),(1/(1.0*power)))
        else:
            print "Input should be of equal length"
        return None

### Euclidean距离 {#euclidean}

令Minkowski距离公式中的p=2，得到Euclidean距离公式：

\\[ d ( {\bf x}, {\bf y} ) = ( \sum_{i=1}^{n} \vert x_i - y_i \vert ^2)^\frac{1}{2} \\]


    def euclidean_distance(x,y):
        return minkowski_distance(x,y,2)

### Manhattan距离 {#manhattan}

令Minkowski距离公式中的p=1，得到Manhattan距离公式：

\\[ d ( {\bf x}, {\bf y} ) = \sum_{i=1}^{n} \vert x_i - y_i \vert \\]


    def euclidean_distance(x,y):
        return minkowski_distance(x,y,1)

### cosine距离 {#cosine}

将Euclidean距离加1，然后取倒数，就可以得到一个介于0到1之间的值，可以用来表示两个向量相似的程度，1表示完全相同。

与此类似，两个向量夹角的余弦值可以更准确地判断这两个向量的相似程度，它反映的是这两个向量与某一条直线的拟合程度，因此也被称为Pearson相关系数，该系数介于-1到1之间，1表示完全正相关，0表示无关，-1表示完全负相关。Pearson相关系数常常用于文本挖掘，词是轴，文档为向量，Pearson系数就代表了这两个文档之间的相似度。

\\[ r ( {\bf x}, {\bf y} ) = \frac{\sum xy - \frac{\sum x \sum y}{n}}{\sqrt{(\sum x^2 - \frac{(\sum x)^2}{n})(\sum y^2 - \frac{(\sum y)^2}{n})}}\\]


    def cosine_distance(x,y):
        if len(x) == len(y):
            return np.dot(x,y)/np.sqrt(np.dot(x,x)*np.dot(y,y))
        else:
            print "Input should be of equal length"
        return None

对于矩阵a，`np.corrcoef(a)`和`np.corrcoef(a,rowvar=0)`可以计算各行和各列之间的相关系数，如：

    >>> a = [[1, 2, 3, 4, 5],
            [8, 3, 4, 2, 6],
            [9, 5, 7, 2, 5]]
    >>> np.corrcoef(a)
    array([[ 1.        , -0.32826608, -0.66697297],
          [-0.32826608,  1.        ,  0.80412382],
          [-0.66697297,  0.80412382,  1.        ]])

### Jaccard距离 {#jaccard}

Jaccard距离属于非欧空间距离。两个集合的交集与并集的比值称为Jaccard系数，Jaccard系数可用于衡量聚类算法的性能。1减去Jaccard系数就是Jaccard距离。


    def jaccard_distance(x,y):
        set_x = set(x)
        set_y = set(y)
        return 1-len(set_x.intersection(set_y)) / len(set_x.union(set_y))

### 数据的归一化 {#norm}

对于不同的特征、属性，其取值可能相差很大，如年龄的取值一般为0到100，而行驶里程有可能在0到几万甚至几十万之间取值，这会影响距离计算结果的合理性。因此，有时候，需要对数据集进行归一化处理，也就是将所有特征、属性的取值范围转化在0到1之间。

    def norm(dataset):
        minVals = dataset.min(0)  #选取某一列的最小值
        maxVals = dataset.max(0)
        ranges = maxVals - minVals
        normDataset = np.zeros(np.shape(dataset))
        m = dataset.shape[0]
        normDataset = (dataset - np.tile(minVals,(m,1))) / np.tile(ranges,(m,1))
        return normDataset

### 数据分割 {#split}

kNN的数据集格式通常是这样的：

1. 每一行是一个实例(或记录、样本等)；

2. 每一列表示一个特征(或属性等)；

3. 最后一列是标签(或类别、标记等)。

对于有监督的kNN算法，需要将导入的数据合理分割为训练集和测试集，为使训练集和测试集中的数据随机分布，首先将数据打乱：

    >>> np.random.shuffle(dataset)
    >>> train_size = 0.8
    >>> test_size = 1-train_size
    >>> from sklearn.cross_validation import train_test_split
    >>> train, test = train_test_split(dataset,test_size=test_size)

### 查找k近邻 {#knn_main}

将训练集中的特征矩阵和标签向量分开：

    for indx in train:
        x = train[indx,:-1]
        y = train[indx,-1]

指定k值创建kNN分类器，并开始训练：

    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier(n_neighbors=10)
    knn.fit(x,y)

训练完成后将返回knn对象信息：

    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
       metric_params=None, n_jobs=1, n_neighbors=10, p=2,
       weights='uniform')

这些参数都是可以指定的，如algorithm包含'ball_tree'，'kd_tree'，'brute'以及'auto'，leaf_size是一个整数，用于指定'ball_tree'、'kd_tree'叶节点的数，metric用于指定距离度量方式，n_jobs指定并行计算线程数量，-1表示CPU的内核数，n_neighbors指定k值，p指定Minkowski距离中的p值，weights是权重，'uniform'表示等比重，'distance'表示按距离反比等等。

这时候可以查看当给定x向量时，模型预测的标签：

    y_predicted = knn_model.predict(x)

对照模型预测标签和真实标签，就可以评价该模型的性能：

    from sklearn.metrics import classification_report
    print classification_report(y,y_predicted)

结果是类似这样的报告形式：

            precision    recall  f1-score   support

    class 0       0.50      1.00      0.67         1
    class 1       0.00      0.00      0.00         1
    class 2       1.00      0.67      0.80         3

    avg / total       0.70      0.60      0.61         5

其中precision表示真实标签与真实标签加漏报标签之间的比值。

查看模型的准确率：

    knn.score(x,y,sample_weight=None)

查看模型的误报情况：

    from sklearn.metrics import confusion_matrix
    print confusion_matrix(y,y_predicted)

输出为一个矩阵(Confusion Matrix)：

    array([[80,  0, 10],
           [ 0, 85,  5],
           [ 1,  6, 83]])

这个矩阵可以直观地看到模型的误报情况，以第二行为例，表示将class2误报为class3的数目为5，误报为class1的数目为0。

计算预测结果的概率分布：

    knn.predict_proba(x)

计算距离测试样本最近的10个点，返回的是这些实例所在的行序号：

    knn.kneighbors(x,10,False)

第三个参数是'return_distance'，指定是否返回具体的距离值。

**[[TOP](#top)]**

***

## 贝叶斯分类器 {#bayes}

### 生成式模型 {#generative}

贝叶斯分类器的理论基础是贝叶斯公式：

\\[ P ( c \vert {\bf x } ) = \frac{ P ( {\bf x } \vert c ) P ( c ) }{ P ( {\bf x } ) }\\]

这里的P(c)为先验概率，P(x\|c)是样本x相对于类标记c的类条件概率，P(x)是用于归一化的因子，对于给定的样本x，P(x)与类标记无关，因此，估计P(c\|x)的问题就转化为计算P(x\|c)P(c)。贝叶斯分类器与决策树、神经网络、支持向量机等“判别式模型”的不同之处就在于它是一种间接判断模式，被称为“生成式模型”。

如果x中各实例的特征、属性之间相互独立，即每个特征、属性**独立**影响分类结果，则有

\\[ P ( {\bf x } \vert c ) = \prod ^{t}_{i=1} P ( x_i \vert c ) \\]

此时的分类器称为朴素贝叶斯分类器。

对\\(P( x_i \vert c)\\)的不同假设，朴素贝叶斯分类器可分为高斯朴素贝叶斯(Gaussian Naive Bayes)，多项式朴素贝叶斯(Multinomial Naive Bayes)和伯努利朴素贝叶斯(Bernoulli Naive Bayes)。

下面尝试使用贝叶斯分类器对文本进行分类过滤。

### 文本向量化 {#text2vec}

对文本进行分词(这里以英文为例，中文可使用[结巴分词](https://github.com/fxsjy/jieba){:target="_blank"})，每个词作为一个元素，组成列表。关于文本处理，可查看[这里](http://about.uuspider.com/2016/04/08/pyre.html){:target="_blank"}。

    sentence = 'today is your last day'
    s_vec = [x.strip() for x in sentence.split()]
    words_li = [['today', 'is', 'your', 'last', 'day'],
               ['Police', 'have', 'been', 'unable', 'to', 'tell', 'the', 'age'],
               ['We', 'will', 'be', 'keeping', 'this', 'part', 'updated'],
               ['Tis', 'the', 'season', 'to', 'buy', 'votives'],
               ['the', 'lender', 'sends', 'that', 'to', 'the', 'federal', 'bank', 'database'],
               ['Shop', 'service', 'pride', 'items', 'for', 'every', 'branch', 'of', 'service']]

作为训练集，对每一句话的单词列表都进行人工标记，以确定其所属类别：

    class_vec = [1,0,0,1,0,1]

其中1表示来自于spam，0是正常文本。

生成一个单词的集合，并转换为列表：

    words_set = set([])
    for doc in words_li:
        words_set = words_set | set(doc)
    all_words_li = list(words_set)

将每一个单词列表都转成向量：

    words_matrix = []
    for i in range(len(words_li)):
        words_vec = [0]*len(all_words_li)
        for word in words_li[i]:
            if word in all_words_li:
                words_vec[all_words_li.index(word)] = 1
        words_matrix.append(words_vec)

最后将以单词向量为元素，整个训练集就转换为一个嵌套列表(矩阵)，记为words_matrix。

### 高斯朴素贝叶斯 {#bayes_g}

高斯朴素贝叶斯假设\\(P( x_i \vert c)\\)为正态分布：

\\[ P ( x_i \vert c ) = \frac{1}{\sqrt{2 \pi \sigma ^2 _c}} \exp(-\frac{(x_i - \mu _c)^2}{2 \sigma ^2 _c})\\]

其中\\(\mu _c\\)和\\(\sigma ^2 _c\\)需要从训练集中估计。

    from sklearn.naive_bayes import GaussianNB
    gnb = GaussianNB()
    gnb.fit(words_matrix,class_vec)

如果数据量非常大，可以使用partial_fit将训练集分成多次载入内存。

对新的输入文本进行分类，如：

    new_words_li = ['no', 'news', 'is', 'good', 'news']

将其转为向量new_words_vec，再转为矩阵new_words_matrix后：

    print gnb.predict(new_words_matrix)

返回1表示为spam，返回0为正常文本。

    print gnb.predict_proba(new_words_matrix)

返回对应于不同类别的概率。

    print gnb.predict_log_proba(new_words_matrix)

返回对应于不同类别概率的对数值。

### 多项式朴素贝叶斯 {#bayes_m}

多项式朴素贝叶斯假设\\(P( x_i \vert c)\\)为多项式分布：

\\[ P ( x_i \vert c ) = \frac{ x_i + \lambda }{ m + n \lambda }\\]

    from sklearn.naive_bayes import MultinomialNB
    mnb = MultinomialNB()
    mnb.fit(words_matrix,class_vec)

`MultinomialNB()`有3个参数，alpha对应于公式中的\\(\lambda\\)，取值通常为1，fit_prior是一个布尔值，表示是否考虑先验概率，当取值为True时，可设定class_prior指定先验概率。

`MultinomialNB()`的`fit`，`predict`，`predict_proba`等用法与`GaussianNB()`完全相同。

### 伯努利朴素贝叶斯 {#bayes_b}

伯努利朴素贝叶斯假设\\(P( x_i \vert c)\\)为二元伯努利分布：

\\[ P ( x_i \vert c ) = P ( i \vert c ) x_i + ( 1 - P ( i \vert c ))( 1 - x_i )\\]

    from sklearn.naive_bayes import BernoulliNB
    bnb = BernoulliNB()
    bnb.fit(words_matrix,class_vec)

`BernoulliNB()`的`fit`，`predict`，`predict_proba`等用法与`GaussianNB()`完全相同。

**[[TOP](#top)]**

***

## 决策树 {#decision_tree}

### 信息增益、增益率和基尼指数 {#entropy}

在给定的样本集合中，每个实例可能属于不同的类别，衡量集合纯度最常用的指标就是熵(entropy)，其计算公式为：

\\[ Ent ( X ) = - \Sigma ^n _{i=1} P ( x _i ) \log_2 P ( x _i ) \\]

熵越小，表示集合的纯度越高，如果集合所有元素属于同一类，则：

\\[ Ent ( X ) = - \Sigma ^n _{i=1} 1 \log_2 1 = 0 \\]

信息增益可用于表征使用特征(属性)a划分集合时引起的纯度提升，计算公式为：

\\[ Gain( X, a ) = Ent ( X ) - \Sigma ^v _{ \upsilon = 1 } \frac{ \vert X^{\upsilon} \vert }{ \vert X \vert } Ent( X^{\upsilon} ) \\]

v是特征(属性)a所有可能取值的数目，每一个取值产生一个类。

增益率(gain ratio)是另外一种衡量集合纯度变化的指标，其计算公式为：

\\[ Gain_ratio ( X, a ) = \frac{ Gain( X, a )}{ IV(a) } \\]

其中：

\\[ IV(a) = - \Sigma ^v _{ \upsilon = 1 } \frac{ \vert X^{\upsilon} \vert }{ \vert X \vert } \log_2 \frac{ \vert X^{\upsilon} \vert }{ \vert X \vert } \\]

是特征(属性)a的固有熵值(intrinsic value)，该值越大，表示特征(属性)a的可能取值数目越多。

基尼值也可用于表征数据集的纯度：

\\[ Gini(X) = \Sigma ^{\vert y \vert} _{i=1} \Sigma _{j \neq i} P ( x _i ) P ( x _j ) = 1 - \Sigma ^{\vert y \vert} _{i=1} (P ( x _i ))^2 \\]

Gini(X)反映了从数据集中随机抽取两个实例，其类别不一致的概率，Gini(X)越小，数据集的纯度越高。

特征(属性)a的基尼指数的计算公式为：

\\[ Gini\_index(X,a) = \Sigma ^v _{ \upsilon =1} \frac{ \vert X^{\upsilon} \vert }{ \vert X \vert } Gini( X^{\upsilon} ) \\]

采用信息增益、增益率和基尼指数的决策树分别为ID3、C4.5和CART算法。下面即将讲述的`sklearn.tree.DecisionTreeClassifier`使用的是优化的CART，可以设置`criterion`参数为`entropy`，即信息增益，就近似为ID3了，C4.5基于信息增益率，`sklearn.tree.DecisionTreeClassifier`不能直接实现。






**[[TOP](#top)]**

***
