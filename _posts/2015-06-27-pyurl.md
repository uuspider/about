---
layout: post
title: urllib & urllib2
---
## python 2: urllib & urllib2

参考：[urllib - open arbitrary resources by URL][ref1]{:target="_blank"}, [urllib2 - extensible library for opening URLs][ref2]{:target="_blank"}

[ref1]:https://docs.python.org/2/library/urllib.html
[ref2]:https://docs.python.org/2/library/urllib2.html

<h2 id="top"></h2>

***

*   [sys & os](#sys)
*   [文件操作](#io)
    *   [基本操作](#rw)
    *   [按行迭代](#readline)
*   [字符串处理](#string)
*   [正则表达式](#re)
    *   [基本规则](#basic): [元字符](#element)\|[计数](#count)\|[界定](#arrow)\|[组](#group)
    *   [函数](#func)
    *   [对象](#obj): [pattern对象](#pattern)\|[match对象](#match)

***

本文需要具备http、cookies等方面的基础知识，快速入门请查看[这里](http://about.uuspider.com/2015/07/25/curl.html)。

urllib和urllib2是python2标准库中的两个模块，可以通过

    >>> help(urllib)
    >>> help(urllib2)

查看模块中的函数、变量等详细信息。

## 基本功能 {#basic}

    >>> w = urllib.urlopen("http://www.xinhuanet.com/")
    >>> print w.read()

抓取的网页可以直接进行解析，也可以保存为page.html：

    >>> f = open('./page.html', 'w')
    >>> f.write(w)
    >>> f.close()

解析网页可以采用[文本处理](http://about.uuspider.com/2016/04/08/pyre.html)的方式，还可以使用[Beautiful Soup等专用模块](http://about.uuspider.com/2015/08/04/beautifulsoup.html)。

涉及到编码和解码，可参考[这里](http://about.uuspider.com/2015/07/20/decode.html#decode)。

### Request

为指定http request头信息，特引入Request类：

    >>> request = urllib2.Request("http://www.xinhuanet.com")
    >>> response = urllib2.urlopen(request)
    >>> print response.read()

### POST & GET

关于POST和GET方法，请查看[这里](http://about.uuspider.com/2015/07/25/curl.html#form)。下面仅列出python的处理方式。

#### POST

    >>> values = {"username": "NAME", "passwd":"PASSWD"} #将需要POST的数据定义为一个字典
    >>> data = urllib.urlencode(values) #将定义的字典编码
    >>> url = "https://login.uuspider.com/"
    >>> request = urllib2.Request(url, data)
    >>> response = urllib2.urlopen(request)
    >>> print resposne.read()

#### GET

    >>> values = {"username": "NAME", "passwd":"PASSWD"} #将需要POST的数据定义为一个字典
    >>> data = urllib.urlencode(values) #将定义的字典编码
    >>> url = "https://login.uuspider.com/"
    >>> url_get = url + "?" + data #构造GET方法的url
    >>> request = urllib2.Request(url_get)
    >>> response = urllib2.urlopen(request)
    >>> print resposne.read()

### 设置headers

    >>> values = {"username": "NAME", "passwd":"PASSWD"} #将需要POST的数据定义为一个字典
    >>> data = urllib.urlencode(values) #将定义的字典编码
    >>> url = "https://login.uuspider.com/"
    >>> user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.130 Safari/537.36' #设置user-agent
    >>> referer = 'http://about.uuspider.com/' #设置referer
    >>> headers = {'User-Agent' : user_agent, 'Referer' : referer} #构造headers头信息
    >>> request = urllib2.Request(url, data, headers)
    >>> response = urllib2.urlopen(request)
    >>> print resposne.read()

### 代理服务器

    >>> proxies = {'http': 'http://127.0.0.1:8087', 'https': 'https://127.0.0.1:8087'}
    >>> proxy_handler = urllib2.ProxyHandler(proxies)
    >>> opener = urllib2.build_opener(proxy_handler)
    >>> response = opener.open('https://www.google.com')
    >>> print response.read()

也可以这样使用代理：

    >>> response = urllib.urlopen('https://www.google.com', proxies = proxies)
    >>> print response.read()

### timeout

    >>> response = urllib2.urlopen('http://www.uuspider.com', timeout = 15)

## urlopen()对象

    >>> print response.read()
    >>> print response.readline()
    >>> print response.readlines()
    >>> print response.fileno()
    >>> print response.info() #返回response头信息
    >>> print response.getcode() #返回http状态码
    >>> print response.geturl() #返回请求的url
    >>> print response.close()

## 异常处理

    try:
        data = urllib.request.urlopen(url)
        print(data.read().decode('utf-8'))
    except urllib.error.HTTPError as e:
        print(e.code)
    except urllib.error.URLError as e:
        print(e.reason)

**[[TOP](#top)]**

***